{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {
        "id": "6yUKoL5OMTZ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 433,
      "metadata": {},
      "outputs": [],
      "source": [
        "diabetes_df = pd.read_csv(\"diabetic_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5j_Xsy6MTZ_",
        "outputId": "b30178c4-e135-4297-b2ee-9ada54722e52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([85.,  3., 58.,  3., 20.,  0.,  0.,  0.,  9.]), tensor(0.))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class ReadmissionPredictionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, diabetes_df: pd.DataFrame, verbose = False):\n",
        "        diabetes_df['readmitted'] = diabetes_df['readmitted'].replace({'<30': 'YES', '>30': 'YES'})\n",
        "        diabetes_df = diabetes_df.drop(columns = 'payer_code')\n",
        "        diabetes_df = diabetes_df.drop(columns = 'patient_nbr')\n",
        "        diabetes_df = diabetes_df.drop(columns = 'medical_specialty')\n",
        "        diabetes_df = diabetes_df.drop(columns = 'encounter_id')\n",
        "        diabetes_df = diabetes_df[diabetes_df['diag_1'] != '?']\n",
        "        diabetes_df = diabetes_df[diabetes_df['diag_2'] != '?']\n",
        "        diabetes_df = diabetes_df[diabetes_df['diag_3'] != '?']\n",
        "        diabetes_df = diabetes_df[diabetes_df['race'] != '?']\n",
        "        diabetes_df = diabetes_df[diabetes_df['weight'] != '?']\n",
        "        diabetes_df = diabetes_df[diabetes_df['admission_type_id'] != 5]\n",
        "        diabetes_df = diabetes_df[diabetes_df['admission_type_id'] != 6]\n",
        "        diabetes_df = diabetes_df[diabetes_df['admission_source_id'] != 17]\n",
        "\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['gender'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['race'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['admission_type_id'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['admission_source_id'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['discharge_disposition_id'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['diag_1'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['diag_2'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['diag_3'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['max_glu_serum'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['A1Cresult'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['metformin'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['repaglinide'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['nateglinide'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['chlorpropamide'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['glimepiride'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['acetohexamide'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['glipizide'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['glyburide'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['tolbutamide'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['pioglitazone'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['rosiglitazone'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['acarbose'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['miglitol'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['troglitazone'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['tolazamide'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['examide'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['citoglipton'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['insulin'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['glyburide-metformin'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['glipizide-metformin'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['glimepiride-pioglitazone'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['metformin-rosiglitazone'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['metformin-pioglitazone'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['change'], prefix='is', prefix_sep='')\n",
        "        diabetes_df = pd.get_dummies(diabetes_df, columns=['diabetesMed'], prefix='is', prefix_sep='')\n",
        "\n",
        "        age_mapping = {\n",
        "            '[0-10)': 5,\n",
        "            '[10-20)': 15,\n",
        "            '[20-30)': 25,\n",
        "            '[30-40)': 35,\n",
        "            '[40-50)': 45,\n",
        "            '[50-60)': 55,\n",
        "            '[60-70)': 65,\n",
        "            '[70-80)': 75,\n",
        "            '[80-90)': 85,\n",
        "            '[90-100)': 95\n",
        "        }\n",
        "        diabetes_df['age'] = diabetes_df['age'].replace(age_mapping)\n",
        "        weight_mapping = {\n",
        "            '[0-25)': 12.5,\n",
        "            '[25-50)': 37.5,\n",
        "            '[50-75)': 62.5,\n",
        "            '[75-100)': 87.5,\n",
        "            '[100-125)': 112.5,\n",
        "            '[125-150)': 137.5,\n",
        "            '[150-175)': 162.5,\n",
        "            '[175-200)': 187.5\n",
        "        }\n",
        "        diabetes_df['weight'] = diabetes_df['weight'].replace(weight_mapping)\n",
        "\n",
        "        readmit_mapping = {\n",
        "            'YES': 1,\n",
        "            'NO': 0\n",
        "        }\n",
        "        diabetes_df['readmitted'] = diabetes_df['readmitted'].replace(readmit_mapping)\n",
        "\n",
        "        # groups = diabetes_df.groupby('readmitted')\n",
        "        # counts = groups.count()\n",
        "        # print(counts)\n",
        "\n",
        "        # unique_values = diabetes_df['diag_1'].unique()\n",
        "        # #print(unique_values)\n",
        "        # groups_type = diabetes_df.groupby('admission_source_id')\n",
        "        # counts_type = groups_type.count()\n",
        "        #print(counts_type)\n",
        "        #counts['encounter_id'].plot.bar()\n",
        "\n",
        "        #data_x =  diabetes_df.loc[:, diabetes_df.columns[:len(diabetes_df.columns) - 1]]\n",
        "\n",
        "        data_x = diabetes_df.select_dtypes(include=[int, float]).drop('readmitted', axis=1)\n",
        "\n",
        "        data_y = diabetes_df['readmitted']\n",
        "\n",
        "        self.input = torch.tensor(data_x.values).type(torch.float32)\n",
        "\n",
        "        self.output = torch.tensor(data_y.values).type(torch.float32)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.input[idx], self.output[idx])\n",
        "    \n",
        "train_df, test_df = train_test_split(diabetes_df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = ReadmissionPredictionDataset(train_df, verbose = True)\n",
        "test_dataset = ReadmissionPredictionDataset(test_df)\n",
        "\n",
        "# train_df.head()\n",
        "print(train_dataset[0])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 100, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {
        "id": "JUJcabOYMTaA",
        "outputId": "f956a61c-de7f-4de3-b241-30500f66afc6"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(nn.Linear(9,64), nn.Sigmoid(), nn.Linear(64,128), nn.Sigmoid(), nn.Linear(128, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        yhat = self.layers(x)\n",
        "        return yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_network(model, train_loader, criterion, optimizer, nepoch=100):\n",
        "    try:\n",
        "        for epoch in tqdm(range(nepoch)):\n",
        "            print('EPOCH %d'%epoch)\n",
        "            total_loss = 0\n",
        "            count = 0\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.squeeze(), labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                count += 1\n",
        "            print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting from training early')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_network(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    true, pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels  in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.round(outputs).squeeze()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            true.append(labels)\n",
        "            pred.append(predicted)\n",
        "    acc = (100 * correct / total)\n",
        "    print('accuracy: %0.3f' % (acc))\n",
        "    true = np.concatenate(true)\n",
        "    pred = np.concatenate(pred)\n",
        "    return acc, true, pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SimpleNet()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 446,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7137f059b38446ed8f68475c481595de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0\n",
            " Train loss: 0.59280\n",
            "EPOCH 1\n",
            " Train loss: 0.58838\n",
            "EPOCH 2\n",
            " Train loss: 0.58685\n",
            "EPOCH 3\n",
            " Train loss: 0.58882\n",
            "EPOCH 4\n",
            " Train loss: 0.58601\n",
            "EPOCH 5\n",
            " Train loss: 0.58874\n",
            "EPOCH 6\n",
            " Train loss: 0.58654\n",
            "EPOCH 7\n",
            " Train loss: 0.58656\n",
            "EPOCH 8\n",
            " Train loss: 0.58770\n",
            "EPOCH 9\n",
            " Train loss: 0.58790\n",
            "EPOCH 10\n",
            " Train loss: 0.58781\n",
            "EPOCH 11\n",
            " Train loss: 0.58568\n",
            "EPOCH 12\n",
            " Train loss: 0.58823\n",
            "EPOCH 13\n",
            " Train loss: 0.58544\n",
            "EPOCH 14\n",
            " Train loss: 0.58487\n",
            "EPOCH 15\n",
            " Train loss: 0.58787\n",
            "EPOCH 16\n",
            " Train loss: 0.58388\n",
            "EPOCH 17\n",
            " Train loss: 0.58542\n",
            "EPOCH 18\n",
            " Train loss: 0.58578\n",
            "EPOCH 19\n",
            " Train loss: 0.58554\n",
            "EPOCH 20\n",
            " Train loss: 0.58598\n",
            "EPOCH 21\n",
            " Train loss: 0.58445\n",
            "EPOCH 22\n",
            " Train loss: 0.59545\n",
            "EPOCH 23\n",
            " Train loss: 0.58911\n",
            "EPOCH 24\n",
            " Train loss: 0.58522\n",
            "EPOCH 25\n",
            " Train loss: 0.58464\n",
            "EPOCH 26\n",
            " Train loss: 0.58464\n",
            "EPOCH 27\n",
            " Train loss: 0.58440\n",
            "EPOCH 28\n",
            " Train loss: 0.58344\n",
            "EPOCH 29\n",
            " Train loss: 0.58447\n",
            "EPOCH 30\n",
            " Train loss: 0.58647\n",
            "EPOCH 31\n",
            " Train loss: 0.58788\n",
            "EPOCH 32\n",
            " Train loss: 0.58508\n",
            "EPOCH 33\n",
            " Train loss: 0.58632\n",
            "EPOCH 34\n",
            " Train loss: 0.58426\n",
            "EPOCH 35\n",
            " Train loss: 0.58422\n",
            "EPOCH 36\n",
            " Train loss: 0.58679\n",
            "EPOCH 37\n",
            " Train loss: 0.58405\n",
            "EPOCH 38\n",
            " Train loss: 0.58214\n",
            "EPOCH 39\n",
            " Train loss: 0.58370\n",
            "EPOCH 40\n",
            " Train loss: 0.58951\n",
            "EPOCH 41\n",
            " Train loss: 0.58364\n",
            "EPOCH 42\n",
            " Train loss: 0.58272\n",
            "EPOCH 43\n",
            " Train loss: 0.58279\n",
            "EPOCH 44\n",
            " Train loss: 0.58200\n",
            "EPOCH 45\n",
            " Train loss: 0.58398\n",
            "EPOCH 46\n",
            " Train loss: 0.58290\n",
            "EPOCH 47\n",
            " Train loss: 0.58314\n",
            "EPOCH 48\n",
            " Train loss: 0.58569\n",
            "EPOCH 49\n",
            " Train loss: 0.58276\n",
            "EPOCH 50\n",
            " Train loss: 0.58418\n",
            "EPOCH 51\n",
            " Train loss: 0.58727\n",
            "EPOCH 52\n",
            " Train loss: 0.58119\n",
            "EPOCH 53\n",
            " Train loss: 0.58253\n",
            "EPOCH 54\n",
            " Train loss: 0.58543\n",
            "EPOCH 55\n",
            " Train loss: 0.58444\n",
            "EPOCH 56\n",
            " Train loss: 0.58142\n",
            "EPOCH 57\n",
            " Train loss: 0.58068\n",
            "EPOCH 58\n",
            " Train loss: 0.58287\n",
            "EPOCH 59\n",
            " Train loss: 0.57984\n",
            "EPOCH 60\n",
            " Train loss: 0.58350\n",
            "EPOCH 61\n",
            " Train loss: 0.58154\n",
            "EPOCH 62\n",
            " Train loss: 0.58683\n",
            "EPOCH 63\n",
            " Train loss: 0.58230\n",
            "EPOCH 64\n",
            " Train loss: 0.57973\n",
            "EPOCH 65\n",
            " Train loss: 0.58124\n",
            "EPOCH 66\n",
            " Train loss: 0.58148\n",
            "EPOCH 67\n",
            " Train loss: 0.58123\n",
            "EPOCH 68\n",
            " Train loss: 0.58154\n",
            "EPOCH 69\n",
            " Train loss: 0.58009\n",
            "EPOCH 70\n",
            " Train loss: 0.58023\n",
            "EPOCH 71\n",
            " Train loss: 0.58137\n",
            "EPOCH 72\n",
            " Train loss: 0.57986\n",
            "EPOCH 73\n",
            " Train loss: 0.57893\n",
            "EPOCH 74\n",
            " Train loss: 0.57968\n",
            "EPOCH 75\n",
            " Train loss: 0.58070\n",
            "EPOCH 76\n",
            " Train loss: 0.58319\n",
            "EPOCH 77\n",
            " Train loss: 0.58132\n",
            "EPOCH 78\n",
            " Train loss: 0.58094\n",
            "EPOCH 79\n",
            " Train loss: 0.58105\n",
            "EPOCH 80\n",
            " Train loss: 0.58080\n",
            "EPOCH 81\n",
            " Train loss: 0.58045\n",
            "EPOCH 82\n",
            " Train loss: 0.58129\n",
            "EPOCH 83\n",
            " Train loss: 0.58237\n",
            "EPOCH 84\n",
            " Train loss: 0.57998\n",
            "EPOCH 85\n",
            " Train loss: 0.57903\n",
            "EPOCH 86\n",
            " Train loss: 0.57854\n",
            "EPOCH 87\n",
            " Train loss: 0.57904\n",
            "EPOCH 88\n",
            " Train loss: 0.58057\n",
            "EPOCH 89\n",
            " Train loss: 0.58133\n",
            "EPOCH 90\n",
            " Train loss: 0.57992\n",
            "EPOCH 91\n",
            " Train loss: 0.57886\n",
            "EPOCH 92\n",
            " Train loss: 0.57938\n",
            "EPOCH 93\n",
            " Train loss: 0.58059\n",
            "EPOCH 94\n",
            " Train loss: 0.57751\n",
            "EPOCH 95\n",
            " Train loss: 0.57893\n",
            "EPOCH 96\n",
            " Train loss: 0.58235\n",
            "EPOCH 97\n",
            " Train loss: 0.57786\n",
            "EPOCH 98\n",
            " Train loss: 0.57873\n",
            "EPOCH 99\n",
            " Train loss: 0.57759\n",
            "EPOCH 100\n",
            " Train loss: 0.57836\n",
            "EPOCH 101\n",
            " Train loss: 0.58046\n",
            "EPOCH 102\n",
            " Train loss: 0.57771\n",
            "EPOCH 103\n",
            " Train loss: 0.57591\n",
            "EPOCH 104\n",
            " Train loss: 0.57608\n",
            "EPOCH 105\n",
            " Train loss: 0.58077\n",
            "EPOCH 106\n",
            " Train loss: 0.57988\n",
            "EPOCH 107\n",
            " Train loss: 0.58019\n",
            "EPOCH 108\n",
            " Train loss: 0.58045\n",
            "EPOCH 109\n",
            " Train loss: 0.57756\n",
            "EPOCH 110\n",
            " Train loss: 0.57605\n",
            "EPOCH 111\n",
            " Train loss: 0.57908\n",
            "EPOCH 112\n",
            " Train loss: 0.57722\n",
            "EPOCH 113\n",
            " Train loss: 0.57748\n",
            "EPOCH 114\n",
            " Train loss: 0.57573\n",
            "EPOCH 115\n",
            " Train loss: 0.57586\n",
            "EPOCH 116\n",
            " Train loss: 0.57645\n",
            "EPOCH 117\n",
            " Train loss: 0.57914\n",
            "EPOCH 118\n",
            " Train loss: 0.58522\n",
            "EPOCH 119\n",
            " Train loss: 0.57389\n",
            "EPOCH 120\n",
            " Train loss: 0.57539\n",
            "EPOCH 121\n",
            " Train loss: 0.57550\n",
            "EPOCH 122\n",
            " Train loss: 0.57555\n",
            "EPOCH 123\n",
            " Train loss: 0.57705\n",
            "EPOCH 124\n",
            " Train loss: 0.57428\n",
            "EPOCH 125\n",
            " Train loss: 0.58233\n",
            "EPOCH 126\n",
            " Train loss: 0.57789\n",
            "EPOCH 127\n",
            " Train loss: 0.57477\n",
            "EPOCH 128\n",
            " Train loss: 0.57626\n",
            "EPOCH 129\n",
            " Train loss: 0.57441\n",
            "EPOCH 130\n",
            " Train loss: 0.57694\n",
            "EPOCH 131\n",
            " Train loss: 0.57582\n",
            "EPOCH 132\n",
            " Train loss: 0.57613\n",
            "EPOCH 133\n",
            " Train loss: 0.57726\n",
            "EPOCH 134\n",
            " Train loss: 0.57683\n",
            "EPOCH 135\n",
            " Train loss: 0.57542\n",
            "EPOCH 136\n",
            " Train loss: 0.57388\n",
            "EPOCH 137\n",
            " Train loss: 0.57419\n",
            "EPOCH 138\n",
            " Train loss: 0.57348\n",
            "EPOCH 139\n",
            " Train loss: 0.57444\n",
            "EPOCH 140\n",
            " Train loss: 0.57444\n",
            "EPOCH 141\n",
            " Train loss: 0.58004\n",
            "EPOCH 142\n",
            " Train loss: 0.57392\n",
            "EPOCH 143\n",
            " Train loss: 0.57319\n",
            "EPOCH 144\n",
            " Train loss: 0.57421\n",
            "EPOCH 145\n",
            " Train loss: 0.57531\n",
            "EPOCH 146\n",
            " Train loss: 0.57391\n",
            "EPOCH 147\n",
            " Train loss: 0.57276\n",
            "EPOCH 148\n",
            " Train loss: 0.57211\n",
            "EPOCH 149\n",
            " Train loss: 0.57249\n",
            "EPOCH 150\n",
            " Train loss: 0.57318\n",
            "EPOCH 151\n",
            " Train loss: 0.57251\n",
            "EPOCH 152\n",
            " Train loss: 0.57364\n",
            "EPOCH 153\n",
            " Train loss: 0.57325\n",
            "EPOCH 154\n",
            " Train loss: 0.57460\n",
            "EPOCH 155\n",
            " Train loss: 0.57638\n",
            "EPOCH 156\n",
            " Train loss: 0.56963\n",
            "EPOCH 157\n",
            " Train loss: 0.57104\n",
            "EPOCH 158\n",
            " Train loss: 0.57114\n",
            "EPOCH 159\n",
            " Train loss: 0.57049\n",
            "EPOCH 160\n",
            " Train loss: 0.57185\n",
            "EPOCH 161\n",
            " Train loss: 0.57314\n",
            "EPOCH 162\n",
            " Train loss: 0.57014\n",
            "EPOCH 163\n",
            " Train loss: 0.57119\n",
            "EPOCH 164\n",
            " Train loss: 0.57687\n",
            "EPOCH 165\n",
            " Train loss: 0.57851\n",
            "EPOCH 166\n",
            " Train loss: 0.57337\n",
            "EPOCH 167\n",
            " Train loss: 0.57235\n",
            "EPOCH 168\n",
            " Train loss: 0.57855\n",
            "EPOCH 169\n",
            " Train loss: 0.57877\n",
            "EPOCH 170\n",
            " Train loss: 0.57359\n",
            "EPOCH 171\n",
            " Train loss: 0.57411\n",
            "EPOCH 172\n",
            " Train loss: 0.57657\n",
            "EPOCH 173\n",
            " Train loss: 0.57076\n",
            "EPOCH 174\n",
            " Train loss: 0.56952\n",
            "EPOCH 175\n",
            " Train loss: 0.57047\n",
            "EPOCH 176\n",
            " Train loss: 0.56939\n",
            "EPOCH 177\n",
            " Train loss: 0.57464\n",
            "EPOCH 178\n",
            " Train loss: 0.57114\n",
            "EPOCH 179\n",
            " Train loss: 0.56971\n",
            "EPOCH 180\n",
            " Train loss: 0.57378\n",
            "EPOCH 181\n",
            " Train loss: 0.56967\n",
            "EPOCH 182\n",
            " Train loss: 0.57022\n",
            "EPOCH 183\n",
            " Train loss: 0.56858\n",
            "EPOCH 184\n",
            " Train loss: 0.57108\n",
            "EPOCH 185\n",
            " Train loss: 0.57382\n",
            "EPOCH 186\n",
            " Train loss: 0.57146\n",
            "EPOCH 187\n",
            " Train loss: 0.57059\n",
            "EPOCH 188\n",
            " Train loss: 0.56881\n",
            "EPOCH 189\n",
            " Train loss: 0.57361\n",
            "EPOCH 190\n",
            " Train loss: 0.56802\n",
            "EPOCH 191\n",
            " Train loss: 0.57046\n",
            "EPOCH 192\n",
            " Train loss: 0.56779\n",
            "EPOCH 193\n",
            " Train loss: 0.56822\n",
            "EPOCH 194\n",
            " Train loss: 0.56747\n",
            "EPOCH 195\n",
            " Train loss: 0.57031\n",
            "EPOCH 196\n",
            " Train loss: 0.56999\n",
            "EPOCH 197\n",
            " Train loss: 0.56965\n",
            "EPOCH 198\n",
            " Train loss: 0.56838\n",
            "EPOCH 199\n",
            " Train loss: 0.56801\n",
            "EPOCH 200\n",
            " Train loss: 0.56778\n",
            "EPOCH 201\n",
            " Train loss: 0.56710\n",
            "EPOCH 202\n",
            " Train loss: 0.56769\n",
            "EPOCH 203\n",
            " Train loss: 0.57718\n",
            "EPOCH 204\n",
            " Train loss: 0.58044\n",
            "EPOCH 205\n",
            " Train loss: 0.57145\n",
            "EPOCH 206\n",
            " Train loss: 0.56736\n",
            "EPOCH 207\n",
            " Train loss: 0.56505\n",
            "EPOCH 208\n",
            " Train loss: 0.56859\n",
            "EPOCH 209\n",
            " Train loss: 0.56635\n",
            "EPOCH 210\n",
            " Train loss: 0.56764\n",
            "EPOCH 211\n",
            " Train loss: 0.56788\n",
            "EPOCH 212\n",
            " Train loss: 0.56774\n",
            "EPOCH 213\n",
            " Train loss: 0.56733\n",
            "EPOCH 214\n",
            " Train loss: 0.57040\n",
            "EPOCH 215\n",
            " Train loss: 0.56936\n",
            "EPOCH 216\n",
            " Train loss: 0.56641\n",
            "EPOCH 217\n",
            " Train loss: 0.56571\n",
            "EPOCH 218\n",
            " Train loss: 0.56745\n",
            "EPOCH 219\n",
            " Train loss: 0.56716\n",
            "EPOCH 220\n",
            " Train loss: 0.56624\n",
            "EPOCH 221\n",
            " Train loss: 0.56583\n",
            "EPOCH 222\n",
            " Train loss: 0.56432\n",
            "EPOCH 223\n",
            " Train loss: 0.56652\n",
            "EPOCH 224\n",
            " Train loss: 0.56561\n",
            "EPOCH 225\n",
            " Train loss: 0.56608\n",
            "EPOCH 226\n",
            " Train loss: 0.56619\n",
            "EPOCH 227\n",
            " Train loss: 0.56817\n",
            "EPOCH 228\n",
            " Train loss: 0.57060\n",
            "EPOCH 229\n",
            " Train loss: 0.56532\n",
            "EPOCH 230\n",
            " Train loss: 0.56757\n",
            "EPOCH 231\n",
            " Train loss: 0.56539\n",
            "EPOCH 232\n",
            " Train loss: 0.56581\n",
            "EPOCH 233\n",
            " Train loss: 0.56662\n",
            "EPOCH 234\n",
            " Train loss: 0.56441\n",
            "EPOCH 235\n",
            " Train loss: 0.56532\n",
            "EPOCH 236\n",
            " Train loss: 0.56651\n",
            "EPOCH 237\n",
            " Train loss: 0.56619\n",
            "EPOCH 238\n",
            " Train loss: 0.56570\n",
            "EPOCH 239\n",
            " Train loss: 0.56637\n",
            "EPOCH 240\n",
            " Train loss: 0.56741\n",
            "EPOCH 241\n",
            " Train loss: 0.56748\n",
            "EPOCH 242\n",
            " Train loss: 0.56791\n",
            "EPOCH 243\n",
            " Train loss: 0.56576\n",
            "EPOCH 244\n",
            " Train loss: 0.56613\n",
            "EPOCH 245\n",
            " Train loss: 0.56715\n",
            "EPOCH 246\n",
            " Train loss: 0.56373\n",
            "EPOCH 247\n",
            " Train loss: 0.56291\n",
            "EPOCH 248\n",
            " Train loss: 0.56463\n",
            "EPOCH 249\n",
            " Train loss: 0.56245\n",
            "EPOCH 250\n",
            " Train loss: 0.56913\n",
            "EPOCH 251\n",
            " Train loss: 0.56318\n",
            "EPOCH 252\n",
            " Train loss: 0.56278\n",
            "EPOCH 253\n",
            " Train loss: 0.56376\n",
            "EPOCH 254\n",
            " Train loss: 0.56190\n",
            "EPOCH 255\n",
            " Train loss: 0.56266\n",
            "EPOCH 256\n",
            " Train loss: 0.56300\n",
            "EPOCH 257\n",
            " Train loss: 0.56118\n",
            "EPOCH 258\n",
            " Train loss: 0.56437\n",
            "EPOCH 259\n",
            " Train loss: 0.56076\n",
            "EPOCH 260\n",
            " Train loss: 0.56184\n",
            "EPOCH 261\n",
            " Train loss: 0.56139\n",
            "EPOCH 262\n",
            " Train loss: 0.56138\n",
            "EPOCH 263\n",
            " Train loss: 0.56331\n",
            "EPOCH 264\n",
            " Train loss: 0.56134\n",
            "EPOCH 265\n",
            " Train loss: 0.56198\n",
            "EPOCH 266\n",
            " Train loss: 0.55977\n",
            "EPOCH 267\n",
            " Train loss: 0.56154\n",
            "EPOCH 268\n",
            " Train loss: 0.56055\n",
            "EPOCH 269\n",
            " Train loss: 0.56194\n",
            "EPOCH 270\n",
            " Train loss: 0.56080\n",
            "EPOCH 271\n",
            " Train loss: 0.56373\n",
            "EPOCH 272\n",
            " Train loss: 0.55848\n",
            "EPOCH 273\n",
            " Train loss: 0.56094\n",
            "EPOCH 274\n",
            " Train loss: 0.55904\n",
            "EPOCH 275\n",
            " Train loss: 0.56342\n",
            "EPOCH 276\n",
            " Train loss: 0.56408\n",
            "EPOCH 277\n",
            " Train loss: 0.56524\n",
            "EPOCH 278\n",
            " Train loss: 0.56165\n",
            "EPOCH 279\n",
            " Train loss: 0.56207\n",
            "EPOCH 280\n",
            " Train loss: 0.55986\n",
            "EPOCH 281\n",
            " Train loss: 0.56424\n",
            "EPOCH 282\n",
            " Train loss: 0.56701\n",
            "EPOCH 283\n",
            " Train loss: 0.56057\n",
            "EPOCH 284\n",
            " Train loss: 0.55962\n",
            "EPOCH 285\n",
            " Train loss: 0.56100\n",
            "EPOCH 286\n",
            " Train loss: 0.56047\n",
            "EPOCH 287\n",
            " Train loss: 0.55981\n",
            "EPOCH 288\n",
            " Train loss: 0.56821\n",
            "EPOCH 289\n",
            " Train loss: 0.56568\n",
            "EPOCH 290\n",
            " Train loss: 0.56296\n",
            "EPOCH 291\n",
            " Train loss: 0.55937\n",
            "EPOCH 292\n",
            " Train loss: 0.56095\n",
            "EPOCH 293\n",
            " Train loss: 0.56126\n",
            "EPOCH 294\n",
            " Train loss: 0.55737\n",
            "EPOCH 295\n",
            " Train loss: 0.55860\n",
            "EPOCH 296\n",
            " Train loss: 0.55701\n",
            "EPOCH 297\n",
            " Train loss: 0.55959\n",
            "EPOCH 298\n",
            " Train loss: 0.55809\n",
            "EPOCH 299\n",
            " Train loss: 0.55669\n",
            "EPOCH 300\n",
            " Train loss: 0.56077\n",
            "EPOCH 301\n",
            " Train loss: 0.55950\n",
            "EPOCH 302\n",
            " Train loss: 0.55918\n",
            "EPOCH 303\n",
            " Train loss: 0.55772\n",
            "EPOCH 304\n",
            " Train loss: 0.55960\n",
            "EPOCH 305\n",
            " Train loss: 0.56035\n",
            "EPOCH 306\n",
            " Train loss: 0.55713\n",
            "EPOCH 307\n",
            " Train loss: 0.56074\n",
            "EPOCH 308\n",
            " Train loss: 0.56007\n",
            "EPOCH 309\n",
            " Train loss: 0.56103\n",
            "EPOCH 310\n",
            " Train loss: 0.55804\n",
            "EPOCH 311\n",
            " Train loss: 0.55579\n",
            "EPOCH 312\n",
            " Train loss: 0.55519\n",
            "EPOCH 313\n",
            " Train loss: 0.55653\n",
            "EPOCH 314\n",
            " Train loss: 0.55632\n",
            "EPOCH 315\n",
            " Train loss: 0.55958\n",
            "EPOCH 316\n",
            " Train loss: 0.55751\n",
            "EPOCH 317\n",
            " Train loss: 0.55892\n",
            "EPOCH 318\n",
            " Train loss: 0.55911\n",
            "EPOCH 319\n",
            " Train loss: 0.55727\n",
            "EPOCH 320\n",
            " Train loss: 0.55876\n",
            "EPOCH 321\n",
            " Train loss: 0.55601\n",
            "EPOCH 322\n",
            " Train loss: 0.55665\n",
            "EPOCH 323\n",
            " Train loss: 0.55753\n",
            "EPOCH 324\n",
            " Train loss: 0.55802\n",
            "EPOCH 325\n",
            " Train loss: 0.55643\n",
            "EPOCH 326\n",
            " Train loss: 0.56099\n",
            "EPOCH 327\n",
            " Train loss: 0.55895\n",
            "EPOCH 328\n",
            " Train loss: 0.55628\n",
            "EPOCH 329\n",
            " Train loss: 0.55540\n",
            "EPOCH 330\n",
            " Train loss: 0.55796\n",
            "EPOCH 331\n",
            " Train loss: 0.55908\n",
            "EPOCH 332\n",
            " Train loss: 0.56199\n",
            "EPOCH 333\n",
            " Train loss: 0.55914\n",
            "EPOCH 334\n",
            " Train loss: 0.55566\n",
            "EPOCH 335\n",
            " Train loss: 0.56257\n",
            "EPOCH 336\n",
            " Train loss: 0.55470\n",
            "EPOCH 337\n",
            " Train loss: 0.55550\n",
            "EPOCH 338\n",
            " Train loss: 0.55682\n",
            "EPOCH 339\n",
            " Train loss: 0.56379\n",
            "EPOCH 340\n",
            " Train loss: 0.55558\n",
            "EPOCH 341\n",
            " Train loss: 0.55700\n",
            "EPOCH 342\n",
            " Train loss: 0.55462\n",
            "EPOCH 343\n",
            " Train loss: 0.55363\n",
            "EPOCH 344\n",
            " Train loss: 0.55334\n",
            "EPOCH 345\n",
            " Train loss: 0.55576\n",
            "EPOCH 346\n",
            " Train loss: 0.55479\n",
            "EPOCH 347\n",
            " Train loss: 0.55310\n",
            "EPOCH 348\n",
            " Train loss: 0.55399\n",
            "EPOCH 349\n",
            " Train loss: 0.55300\n",
            "EPOCH 350\n",
            " Train loss: 0.55704\n",
            "EPOCH 351\n",
            " Train loss: 0.55878\n",
            "EPOCH 352\n",
            " Train loss: 0.55910\n",
            "EPOCH 353\n",
            " Train loss: 0.55388\n",
            "EPOCH 354\n",
            " Train loss: 0.55384\n",
            "EPOCH 355\n",
            " Train loss: 0.55933\n",
            "EPOCH 356\n",
            " Train loss: 0.55487\n",
            "EPOCH 357\n",
            " Train loss: 0.55186\n",
            "EPOCH 358\n",
            " Train loss: 0.55466\n",
            "EPOCH 359\n",
            " Train loss: 0.55268\n",
            "EPOCH 360\n",
            " Train loss: 0.55235\n",
            "EPOCH 361\n",
            " Train loss: 0.55837\n",
            "EPOCH 362\n",
            " Train loss: 0.55172\n",
            "EPOCH 363\n",
            " Train loss: 0.55288\n",
            "EPOCH 364\n",
            " Train loss: 0.55444\n",
            "EPOCH 365\n",
            " Train loss: 0.55276\n",
            "EPOCH 366\n",
            " Train loss: 0.55111\n",
            "EPOCH 367\n",
            " Train loss: 0.55327\n",
            "EPOCH 368\n",
            " Train loss: 0.55349\n",
            "EPOCH 369\n",
            " Train loss: 0.55309\n",
            "EPOCH 370\n",
            " Train loss: 0.55338\n",
            "EPOCH 371\n",
            " Train loss: 0.55330\n",
            "EPOCH 372\n",
            " Train loss: 0.55106\n",
            "EPOCH 373\n",
            " Train loss: 0.55275\n",
            "EPOCH 374\n",
            " Train loss: 0.55638\n",
            "EPOCH 375\n",
            " Train loss: 0.55668\n",
            "EPOCH 376\n",
            " Train loss: 0.55150\n",
            "EPOCH 377\n",
            " Train loss: 0.55064\n",
            "EPOCH 378\n",
            " Train loss: 0.55008\n",
            "EPOCH 379\n",
            " Train loss: 0.55150\n",
            "EPOCH 380\n",
            " Train loss: 0.55031\n",
            "EPOCH 381\n",
            " Train loss: 0.56410\n",
            "EPOCH 382\n",
            " Train loss: 0.55445\n",
            "EPOCH 383\n",
            " Train loss: 0.54902\n",
            "EPOCH 384\n",
            " Train loss: 0.55083\n",
            "EPOCH 385\n",
            " Train loss: 0.55077\n",
            "EPOCH 386\n",
            " Train loss: 0.55421\n",
            "EPOCH 387\n",
            " Train loss: 0.55017\n",
            "EPOCH 388\n",
            " Train loss: 0.55094\n",
            "EPOCH 389\n",
            " Train loss: 0.55588\n",
            "EPOCH 390\n",
            " Train loss: 0.54624\n",
            "EPOCH 391\n",
            " Train loss: 0.55334\n",
            "EPOCH 392\n",
            " Train loss: 0.55029\n",
            "EPOCH 393\n",
            " Train loss: 0.54819\n",
            "EPOCH 394\n",
            " Train loss: 0.55344\n",
            "EPOCH 395\n",
            " Train loss: 0.55117\n",
            "EPOCH 396\n",
            " Train loss: 0.55010\n",
            "EPOCH 397\n",
            " Train loss: 0.55192\n",
            "EPOCH 398\n",
            " Train loss: 0.55095\n",
            "EPOCH 399\n",
            " Train loss: 0.54817\n",
            "EPOCH 400\n",
            " Train loss: 0.55379\n",
            "EPOCH 401\n",
            " Train loss: 0.54847\n",
            "EPOCH 402\n",
            " Train loss: 0.54812\n",
            "EPOCH 403\n",
            " Train loss: 0.54707\n",
            "EPOCH 404\n",
            " Train loss: 0.54952\n",
            "EPOCH 405\n",
            " Train loss: 0.55144\n",
            "EPOCH 406\n",
            " Train loss: 0.54850\n",
            "EPOCH 407\n",
            " Train loss: 0.54885\n",
            "EPOCH 408\n",
            " Train loss: 0.55015\n",
            "EPOCH 409\n",
            " Train loss: 0.55202\n",
            "EPOCH 410\n",
            " Train loss: 0.54902\n",
            "EPOCH 411\n",
            " Train loss: 0.54621\n",
            "EPOCH 412\n",
            " Train loss: 0.54686\n",
            "EPOCH 413\n",
            " Train loss: 0.55407\n",
            "EPOCH 414\n",
            " Train loss: 0.54734\n",
            "EPOCH 415\n",
            " Train loss: 0.54729\n",
            "EPOCH 416\n",
            " Train loss: 0.54672\n",
            "EPOCH 417\n",
            " Train loss: 0.55144\n",
            "EPOCH 418\n",
            " Train loss: 0.54564\n",
            "EPOCH 419\n",
            " Train loss: 0.55001\n",
            "EPOCH 420\n",
            " Train loss: 0.55101\n",
            "EPOCH 421\n",
            " Train loss: 0.54762\n",
            "EPOCH 422\n",
            " Train loss: 0.55169\n",
            "EPOCH 423\n",
            " Train loss: 0.55005\n",
            "EPOCH 424\n",
            " Train loss: 0.54803\n",
            "EPOCH 425\n",
            " Train loss: 0.54972\n",
            "EPOCH 426\n",
            " Train loss: 0.55036\n",
            "EPOCH 427\n",
            " Train loss: 0.55203\n",
            "EPOCH 428\n",
            " Train loss: 0.54807\n",
            "EPOCH 429\n",
            " Train loss: 0.54647\n",
            "EPOCH 430\n",
            " Train loss: 0.55153\n",
            "EPOCH 431\n",
            " Train loss: 0.54731\n",
            "EPOCH 432\n",
            " Train loss: 0.54655\n",
            "EPOCH 433\n",
            " Train loss: 0.54870\n",
            "EPOCH 434\n",
            " Train loss: 0.54577\n",
            "EPOCH 435\n",
            " Train loss: 0.54504\n",
            "EPOCH 436\n",
            " Train loss: 0.54437\n",
            "EPOCH 437\n",
            " Train loss: 0.54537\n",
            "EPOCH 438\n",
            " Train loss: 0.54914\n",
            "EPOCH 439\n",
            " Train loss: 0.54858\n",
            "EPOCH 440\n",
            " Train loss: 0.54949\n",
            "EPOCH 441\n",
            " Train loss: 0.54579\n",
            "EPOCH 442\n",
            " Train loss: 0.54958\n",
            "EPOCH 443\n",
            " Train loss: 0.54546\n",
            "EPOCH 444\n",
            " Train loss: 0.54819\n",
            "EPOCH 445\n",
            " Train loss: 0.54712\n",
            "EPOCH 446\n",
            " Train loss: 0.55168\n",
            "EPOCH 447\n",
            " Train loss: 0.54466\n",
            "EPOCH 448\n",
            " Train loss: 0.54350\n",
            "EPOCH 449\n",
            " Train loss: 0.55092\n",
            "EPOCH 450\n",
            " Train loss: 0.54932\n",
            "EPOCH 451\n",
            " Train loss: 0.54338\n",
            "EPOCH 452\n",
            " Train loss: 0.55008\n",
            "EPOCH 453\n",
            " Train loss: 0.54601\n",
            "EPOCH 454\n",
            " Train loss: 0.54634\n",
            "EPOCH 455\n",
            " Train loss: 0.54721\n",
            "EPOCH 456\n",
            " Train loss: 0.54925\n",
            "EPOCH 457\n",
            " Train loss: 0.54745\n",
            "EPOCH 458\n",
            " Train loss: 0.54779\n",
            "EPOCH 459\n",
            " Train loss: 0.54260\n",
            "EPOCH 460\n",
            " Train loss: 0.54699\n",
            "EPOCH 461\n",
            " Train loss: 0.54334\n",
            "EPOCH 462\n",
            " Train loss: 0.54270\n",
            "EPOCH 463\n",
            " Train loss: 0.54243\n",
            "EPOCH 464\n",
            " Train loss: 0.54761\n",
            "EPOCH 465\n",
            " Train loss: 0.54292\n",
            "EPOCH 466\n",
            " Train loss: 0.54444\n",
            "EPOCH 467\n",
            " Train loss: 0.54242\n",
            "EPOCH 468\n",
            " Train loss: 0.54392\n",
            "EPOCH 469\n",
            " Train loss: 0.54134\n",
            "EPOCH 470\n",
            " Train loss: 0.54646\n",
            "EPOCH 471\n",
            " Train loss: 0.54541\n",
            "EPOCH 472\n",
            " Train loss: 0.54185\n",
            "EPOCH 473\n",
            " Train loss: 0.54566\n",
            "EPOCH 474\n",
            " Train loss: 0.54932\n",
            "EPOCH 475\n",
            " Train loss: 0.54437\n",
            "EPOCH 476\n",
            " Train loss: 0.54731\n",
            "EPOCH 477\n",
            " Train loss: 0.54405\n",
            "EPOCH 478\n",
            " Train loss: 0.54600\n",
            "EPOCH 479\n",
            " Train loss: 0.54179\n",
            "EPOCH 480\n",
            " Train loss: 0.54486\n",
            "EPOCH 481\n",
            " Train loss: 0.54288\n",
            "EPOCH 482\n",
            " Train loss: 0.54166\n",
            "EPOCH 483\n",
            " Train loss: 0.54280\n",
            "EPOCH 484\n",
            " Train loss: 0.54228\n",
            "EPOCH 485\n",
            " Train loss: 0.54135\n",
            "EPOCH 486\n",
            " Train loss: 0.54250\n",
            "EPOCH 487\n",
            " Train loss: 0.54381\n",
            "EPOCH 488\n",
            " Train loss: 0.54665\n",
            "EPOCH 489\n",
            " Train loss: 0.54384\n",
            "EPOCH 490\n",
            " Train loss: 0.54496\n",
            "EPOCH 491\n",
            " Train loss: 0.54149\n",
            "EPOCH 492\n",
            " Train loss: 0.54330\n",
            "EPOCH 493\n",
            " Train loss: 0.54248\n",
            "EPOCH 494\n",
            " Train loss: 0.54008\n",
            "EPOCH 495\n",
            " Train loss: 0.54619\n",
            "EPOCH 496\n",
            " Train loss: 0.54079\n",
            "EPOCH 497\n",
            " Train loss: 0.54094\n",
            "EPOCH 498\n",
            " Train loss: 0.54156\n",
            "EPOCH 499\n",
            " Train loss: 0.53820\n",
            "EPOCH 500\n",
            " Train loss: 0.54136\n",
            "EPOCH 501\n",
            " Train loss: 0.53971\n",
            "EPOCH 502\n",
            " Train loss: 0.54478\n",
            "EPOCH 503\n",
            " Train loss: 0.54037\n",
            "EPOCH 504\n",
            " Train loss: 0.54215\n",
            "EPOCH 505\n",
            " Train loss: 0.54169\n",
            "EPOCH 506\n",
            " Train loss: 0.54388\n",
            "EPOCH 507\n",
            " Train loss: 0.53991\n",
            "EPOCH 508\n",
            " Train loss: 0.54437\n",
            "EPOCH 509\n",
            " Train loss: 0.54158\n",
            "EPOCH 510\n",
            " Train loss: 0.54155\n",
            "EPOCH 511\n",
            " Train loss: 0.54925\n",
            "EPOCH 512\n",
            " Train loss: 0.54907\n",
            "EPOCH 513\n",
            " Train loss: 0.53831\n",
            "EPOCH 514\n",
            " Train loss: 0.54517\n",
            "EPOCH 515\n",
            " Train loss: 0.54218\n",
            "EPOCH 516\n",
            " Train loss: 0.53901\n",
            "EPOCH 517\n",
            " Train loss: 0.54049\n",
            "EPOCH 518\n",
            " Train loss: 0.53970\n",
            "EPOCH 519\n",
            " Train loss: 0.54063\n",
            "EPOCH 520\n",
            " Train loss: 0.53810\n",
            "EPOCH 521\n",
            " Train loss: 0.54345\n",
            "EPOCH 522\n",
            " Train loss: 0.53854\n",
            "EPOCH 523\n",
            " Train loss: 0.53908\n",
            "EPOCH 524\n",
            " Train loss: 0.53830\n",
            "EPOCH 525\n",
            " Train loss: 0.53728\n",
            "EPOCH 526\n",
            " Train loss: 0.53838\n",
            "EPOCH 527\n",
            " Train loss: 0.54090\n",
            "EPOCH 528\n",
            " Train loss: 0.53703\n",
            "EPOCH 529\n",
            " Train loss: 0.54161\n",
            "EPOCH 530\n",
            " Train loss: 0.53846\n",
            "EPOCH 531\n",
            " Train loss: 0.53754\n",
            "EPOCH 532\n",
            " Train loss: 0.53980\n",
            "EPOCH 533\n",
            " Train loss: 0.53522\n",
            "EPOCH 534\n",
            " Train loss: 0.53620\n",
            "EPOCH 535\n",
            " Train loss: 0.54327\n",
            "EPOCH 536\n",
            " Train loss: 0.53997\n",
            "EPOCH 537\n",
            " Train loss: 0.53993\n",
            "EPOCH 538\n",
            " Train loss: 0.54545\n",
            "EPOCH 539\n",
            " Train loss: 0.54183\n",
            "EPOCH 540\n",
            " Train loss: 0.53795\n",
            "EPOCH 541\n",
            " Train loss: 0.53808\n",
            "EPOCH 542\n",
            " Train loss: 0.53833\n",
            "EPOCH 543\n",
            " Train loss: 0.53512\n",
            "EPOCH 544\n",
            " Train loss: 0.54011\n",
            "EPOCH 545\n",
            " Train loss: 0.53879\n",
            "EPOCH 546\n",
            " Train loss: 0.53858\n",
            "EPOCH 547\n",
            " Train loss: 0.54093\n",
            "EPOCH 548\n",
            " Train loss: 0.54192\n",
            "EPOCH 549\n",
            " Train loss: 0.53549\n",
            "EPOCH 550\n",
            " Train loss: 0.53765\n",
            "EPOCH 551\n",
            " Train loss: 0.53703\n",
            "EPOCH 552\n",
            " Train loss: 0.53903\n",
            "EPOCH 553\n",
            " Train loss: 0.53763\n",
            "EPOCH 554\n",
            " Train loss: 0.53421\n",
            "EPOCH 555\n",
            " Train loss: 0.53446\n",
            "EPOCH 556\n",
            " Train loss: 0.53767\n",
            "EPOCH 557\n",
            " Train loss: 0.53651\n",
            "EPOCH 558\n",
            " Train loss: 0.53544\n",
            "EPOCH 559\n",
            " Train loss: 0.54184\n",
            "EPOCH 560\n",
            " Train loss: 0.54156\n",
            "EPOCH 561\n",
            " Train loss: 0.53718\n",
            "EPOCH 562\n",
            " Train loss: 0.53774\n",
            "EPOCH 563\n",
            " Train loss: 0.54887\n",
            "EPOCH 564\n",
            " Train loss: 0.53937\n",
            "EPOCH 565\n",
            " Train loss: 0.54072\n",
            "EPOCH 566\n",
            " Train loss: 0.53732\n",
            "EPOCH 567\n",
            " Train loss: 0.53854\n",
            "EPOCH 568\n",
            " Train loss: 0.54030\n",
            "EPOCH 569\n",
            " Train loss: 0.53884\n",
            "EPOCH 570\n",
            " Train loss: 0.53664\n",
            "EPOCH 571\n",
            " Train loss: 0.53682\n",
            "EPOCH 572\n",
            " Train loss: 0.53542\n",
            "EPOCH 573\n",
            " Train loss: 0.53504\n",
            "EPOCH 574\n",
            " Train loss: 0.53383\n",
            "EPOCH 575\n",
            " Train loss: 0.53477\n",
            "EPOCH 576\n",
            " Train loss: 0.53626\n",
            "EPOCH 577\n",
            " Train loss: 0.53269\n",
            "EPOCH 578\n",
            " Train loss: 0.53633\n",
            "EPOCH 579\n",
            " Train loss: 0.53473\n",
            "EPOCH 580\n",
            " Train loss: 0.53315\n",
            "EPOCH 581\n",
            " Train loss: 0.53616\n",
            "EPOCH 582\n",
            " Train loss: 0.53530\n",
            "EPOCH 583\n",
            " Train loss: 0.53687\n",
            "EPOCH 584\n",
            " Train loss: 0.54334\n",
            "EPOCH 585\n",
            " Train loss: 0.53642\n",
            "EPOCH 586\n",
            " Train loss: 0.53237\n",
            "EPOCH 587\n",
            " Train loss: 0.53240\n",
            "EPOCH 588\n",
            " Train loss: 0.53108\n",
            "EPOCH 589\n",
            " Train loss: 0.53389\n",
            "EPOCH 590\n",
            " Train loss: 0.53443\n",
            "EPOCH 591\n",
            " Train loss: 0.53336\n",
            "EPOCH 592\n",
            " Train loss: 0.53437\n",
            "EPOCH 593\n",
            " Train loss: 0.53234\n",
            "EPOCH 594\n",
            " Train loss: 0.53047\n",
            "EPOCH 595\n",
            " Train loss: 0.53861\n",
            "EPOCH 596\n",
            " Train loss: 0.53227\n",
            "EPOCH 597\n",
            " Train loss: 0.53372\n",
            "EPOCH 598\n",
            " Train loss: 0.53601\n",
            "EPOCH 599\n",
            " Train loss: 0.53762\n",
            "EPOCH 600\n",
            " Train loss: 0.53437\n",
            "EPOCH 601\n",
            " Train loss: 0.53250\n",
            "EPOCH 602\n",
            " Train loss: 0.53878\n",
            "EPOCH 603\n",
            " Train loss: 0.53253\n",
            "EPOCH 604\n",
            " Train loss: 0.53300\n",
            "EPOCH 605\n",
            " Train loss: 0.53134\n",
            "EPOCH 606\n",
            " Train loss: 0.53146\n",
            "EPOCH 607\n",
            " Train loss: 0.53367\n",
            "EPOCH 608\n",
            " Train loss: 0.53242\n",
            "EPOCH 609\n",
            " Train loss: 0.53202\n",
            "EPOCH 610\n",
            " Train loss: 0.53378\n",
            "EPOCH 611\n",
            " Train loss: 0.53351\n",
            "EPOCH 612\n",
            " Train loss: 0.53101\n",
            "EPOCH 613\n",
            " Train loss: 0.53580\n",
            "EPOCH 614\n",
            " Train loss: 0.53176\n",
            "EPOCH 615\n",
            " Train loss: 0.52864\n",
            "EPOCH 616\n",
            " Train loss: 0.53227\n",
            "EPOCH 617\n",
            " Train loss: 0.53803\n",
            "EPOCH 618\n",
            " Train loss: 0.53917\n",
            "EPOCH 619\n",
            " Train loss: 0.53537\n",
            "EPOCH 620\n",
            " Train loss: 0.53461\n",
            "EPOCH 621\n",
            " Train loss: 0.52958\n",
            "EPOCH 622\n",
            " Train loss: 0.53257\n",
            "EPOCH 623\n",
            " Train loss: 0.53101\n",
            "EPOCH 624\n",
            " Train loss: 0.52820\n",
            "EPOCH 625\n",
            " Train loss: 0.53885\n",
            "EPOCH 626\n",
            " Train loss: 0.53062\n",
            "EPOCH 627\n",
            " Train loss: 0.53153\n",
            "EPOCH 628\n",
            " Train loss: 0.53384\n",
            "EPOCH 629\n",
            " Train loss: 0.53063\n",
            "EPOCH 630\n",
            " Train loss: 0.53071\n",
            "EPOCH 631\n",
            " Train loss: 0.53138\n",
            "EPOCH 632\n",
            " Train loss: 0.52913\n",
            "EPOCH 633\n",
            " Train loss: 0.53832\n",
            "EPOCH 634\n",
            " Train loss: 0.53124\n",
            "EPOCH 635\n",
            " Train loss: 0.53146\n",
            "EPOCH 636\n",
            " Train loss: 0.53472\n",
            "EPOCH 637\n",
            " Train loss: 0.53318\n",
            "EPOCH 638\n",
            " Train loss: 0.53433\n",
            "EPOCH 639\n",
            " Train loss: 0.53197\n",
            "EPOCH 640\n",
            " Train loss: 0.53313\n",
            "EPOCH 641\n",
            " Train loss: 0.53207\n",
            "EPOCH 642\n",
            " Train loss: 0.53287\n",
            "EPOCH 643\n",
            " Train loss: 0.52920\n",
            "EPOCH 644\n",
            " Train loss: 0.53537\n",
            "EPOCH 645\n",
            " Train loss: 0.53919\n",
            "EPOCH 646\n",
            " Train loss: 0.53087\n",
            "EPOCH 647\n",
            " Train loss: 0.53038\n",
            "EPOCH 648\n",
            " Train loss: 0.52847\n",
            "EPOCH 649\n",
            " Train loss: 0.53197\n",
            "EPOCH 650\n",
            " Train loss: 0.52797\n",
            "EPOCH 651\n",
            " Train loss: 0.52833\n",
            "EPOCH 652\n",
            " Train loss: 0.53670\n",
            "EPOCH 653\n",
            " Train loss: 0.52633\n",
            "EPOCH 654\n",
            " Train loss: 0.52648\n",
            "EPOCH 655\n",
            " Train loss: 0.53055\n",
            "EPOCH 656\n",
            " Train loss: 0.52761\n",
            "EPOCH 657\n",
            " Train loss: 0.53411\n",
            "EPOCH 658\n",
            " Train loss: 0.53169\n",
            "EPOCH 659\n",
            " Train loss: 0.52913\n",
            "EPOCH 660\n",
            " Train loss: 0.52751\n",
            "EPOCH 661\n",
            " Train loss: 0.52813\n",
            "EPOCH 662\n",
            " Train loss: 0.52894\n",
            "EPOCH 663\n",
            " Train loss: 0.53589\n",
            "EPOCH 664\n",
            " Train loss: 0.53004\n",
            "EPOCH 665\n",
            " Train loss: 0.53198\n",
            "EPOCH 666\n",
            " Train loss: 0.52608\n",
            "EPOCH 667\n",
            " Train loss: 0.53179\n",
            "EPOCH 668\n",
            " Train loss: 0.53195\n",
            "EPOCH 669\n",
            " Train loss: 0.52718\n",
            "EPOCH 670\n",
            " Train loss: 0.52784\n",
            "EPOCH 671\n",
            " Train loss: 0.52624\n",
            "EPOCH 672\n",
            " Train loss: 0.52602\n",
            "EPOCH 673\n",
            " Train loss: 0.52904\n",
            "EPOCH 674\n",
            " Train loss: 0.52665\n",
            "EPOCH 675\n",
            " Train loss: 0.52563\n",
            "EPOCH 676\n",
            " Train loss: 0.53043\n",
            "EPOCH 677\n",
            " Train loss: 0.52754\n",
            "EPOCH 678\n",
            " Train loss: 0.52920\n",
            "EPOCH 679\n",
            " Train loss: 0.52531\n",
            "EPOCH 680\n",
            " Train loss: 0.52864\n",
            "EPOCH 681\n",
            " Train loss: 0.53429\n",
            "EPOCH 682\n",
            " Train loss: 0.53494\n",
            "EPOCH 683\n",
            " Train loss: 0.52409\n",
            "EPOCH 684\n",
            " Train loss: 0.52998\n",
            "EPOCH 685\n",
            " Train loss: 0.52999\n",
            "EPOCH 686\n",
            " Train loss: 0.53777\n",
            "EPOCH 687\n",
            " Train loss: 0.52595\n",
            "EPOCH 688\n",
            " Train loss: 0.52457\n",
            "EPOCH 689\n",
            " Train loss: 0.52769\n",
            "EPOCH 690\n",
            " Train loss: 0.53038\n",
            "EPOCH 691\n",
            " Train loss: 0.52596\n",
            "EPOCH 692\n",
            " Train loss: 0.52678\n",
            "EPOCH 693\n",
            " Train loss: 0.52917\n",
            "EPOCH 694\n",
            " Train loss: 0.52466\n",
            "EPOCH 695\n",
            " Train loss: 0.52468\n",
            "EPOCH 696\n",
            " Train loss: 0.52616\n",
            "EPOCH 697\n",
            " Train loss: 0.52394\n",
            "EPOCH 698\n",
            " Train loss: 0.52690\n",
            "EPOCH 699\n",
            " Train loss: 0.52653\n",
            "EPOCH 700\n",
            " Train loss: 0.52523\n",
            "EPOCH 701\n",
            " Train loss: 0.52341\n",
            "EPOCH 702\n",
            " Train loss: 0.52487\n",
            "EPOCH 703\n",
            " Train loss: 0.52559\n",
            "EPOCH 704\n",
            " Train loss: 0.52771\n",
            "EPOCH 705\n",
            " Train loss: 0.52422\n",
            "EPOCH 706\n",
            " Train loss: 0.52799\n",
            "EPOCH 707\n",
            " Train loss: 0.53274\n",
            "EPOCH 708\n",
            " Train loss: 0.52482\n",
            "EPOCH 709\n",
            " Train loss: 0.52337\n",
            "EPOCH 710\n",
            " Train loss: 0.52584\n",
            "EPOCH 711\n",
            " Train loss: 0.52312\n",
            "EPOCH 712\n",
            " Train loss: 0.52627\n",
            "EPOCH 713\n",
            " Train loss: 0.52741\n",
            "EPOCH 714\n",
            " Train loss: 0.52320\n",
            "EPOCH 715\n",
            " Train loss: 0.52429\n",
            "EPOCH 716\n",
            " Train loss: 0.53210\n",
            "EPOCH 717\n",
            " Train loss: 0.52594\n",
            "EPOCH 718\n",
            " Train loss: 0.52833\n",
            "EPOCH 719\n",
            " Train loss: 0.53087\n",
            "EPOCH 720\n",
            " Train loss: 0.52818\n",
            "EPOCH 721\n",
            " Train loss: 0.52537\n",
            "EPOCH 722\n",
            " Train loss: 0.52702\n",
            "EPOCH 723\n",
            " Train loss: 0.52476\n",
            "EPOCH 724\n",
            " Train loss: 0.52803\n",
            "EPOCH 725\n",
            " Train loss: 0.52395\n",
            "EPOCH 726\n",
            " Train loss: 0.52421\n",
            "EPOCH 727\n",
            " Train loss: 0.52430\n",
            "EPOCH 728\n",
            " Train loss: 0.52358\n",
            "EPOCH 729\n",
            " Train loss: 0.52338\n",
            "EPOCH 730\n",
            " Train loss: 0.52575\n",
            "EPOCH 731\n",
            " Train loss: 0.52567\n",
            "EPOCH 732\n",
            " Train loss: 0.52375\n",
            "EPOCH 733\n",
            " Train loss: 0.52546\n",
            "EPOCH 734\n",
            " Train loss: 0.52919\n",
            "EPOCH 735\n",
            " Train loss: 0.52154\n",
            "EPOCH 736\n",
            " Train loss: 0.52991\n",
            "EPOCH 737\n",
            " Train loss: 0.52425\n",
            "EPOCH 738\n",
            " Train loss: 0.52403\n",
            "EPOCH 739\n",
            " Train loss: 0.52224\n",
            "EPOCH 740\n",
            " Train loss: 0.52106\n",
            "EPOCH 741\n",
            " Train loss: 0.52117\n",
            "EPOCH 742\n",
            " Train loss: 0.52567\n",
            "EPOCH 743\n",
            " Train loss: 0.52466\n",
            "EPOCH 744\n",
            " Train loss: 0.52442\n",
            "EPOCH 745\n",
            " Train loss: 0.52477\n",
            "EPOCH 746\n",
            " Train loss: 0.52849\n",
            "EPOCH 747\n",
            " Train loss: 0.52246\n",
            "EPOCH 748\n",
            " Train loss: 0.52481\n",
            "EPOCH 749\n",
            " Train loss: 0.52158\n",
            "EPOCH 750\n",
            " Train loss: 0.52140\n",
            "EPOCH 751\n",
            " Train loss: 0.52170\n",
            "EPOCH 752\n",
            " Train loss: 0.52553\n",
            "EPOCH 753\n",
            " Train loss: 0.52048\n",
            "EPOCH 754\n",
            " Train loss: 0.52463\n",
            "EPOCH 755\n",
            " Train loss: 0.53033\n",
            "EPOCH 756\n",
            " Train loss: 0.52460\n",
            "EPOCH 757\n",
            " Train loss: 0.52576\n",
            "EPOCH 758\n",
            " Train loss: 0.52093\n",
            "EPOCH 759\n",
            " Train loss: 0.52541\n",
            "EPOCH 760\n",
            " Train loss: 0.52316\n",
            "EPOCH 761\n",
            " Train loss: 0.52232\n",
            "EPOCH 762\n",
            " Train loss: 0.52280\n",
            "EPOCH 763\n",
            " Train loss: 0.52029\n",
            "EPOCH 764\n",
            " Train loss: 0.52105\n",
            "EPOCH 765\n",
            " Train loss: 0.52202\n",
            "EPOCH 766\n",
            " Train loss: 0.51994\n",
            "EPOCH 767\n",
            " Train loss: 0.51971\n",
            "EPOCH 768\n",
            " Train loss: 0.52135\n",
            "EPOCH 769\n",
            " Train loss: 0.52234\n",
            "EPOCH 770\n",
            " Train loss: 0.52675\n",
            "EPOCH 771\n",
            " Train loss: 0.52493\n",
            "EPOCH 772\n",
            " Train loss: 0.52146\n",
            "EPOCH 773\n",
            " Train loss: 0.52340\n",
            "EPOCH 774\n",
            " Train loss: 0.51954\n",
            "EPOCH 775\n",
            " Train loss: 0.51840\n",
            "EPOCH 776\n",
            " Train loss: 0.52068\n",
            "EPOCH 777\n",
            " Train loss: 0.52128\n",
            "EPOCH 778\n",
            " Train loss: 0.52192\n",
            "EPOCH 779\n",
            " Train loss: 0.51705\n",
            "EPOCH 780\n",
            " Train loss: 0.52692\n",
            "EPOCH 781\n",
            " Train loss: 0.51988\n",
            "EPOCH 782\n",
            " Train loss: 0.51947\n",
            "EPOCH 783\n",
            " Train loss: 0.52260\n",
            "EPOCH 784\n",
            " Train loss: 0.52425\n",
            "EPOCH 785\n",
            " Train loss: 0.52291\n",
            "EPOCH 786\n",
            " Train loss: 0.52045\n",
            "EPOCH 787\n",
            " Train loss: 0.51967\n",
            "EPOCH 788\n",
            " Train loss: 0.52229\n",
            "EPOCH 789\n",
            " Train loss: 0.51913\n",
            "EPOCH 790\n",
            " Train loss: 0.51880\n",
            "EPOCH 791\n",
            " Train loss: 0.52267\n",
            "EPOCH 792\n",
            " Train loss: 0.51823\n",
            "EPOCH 793\n",
            " Train loss: 0.52180\n",
            "EPOCH 794\n",
            " Train loss: 0.51798\n",
            "EPOCH 795\n",
            " Train loss: 0.51877\n",
            "EPOCH 796\n",
            " Train loss: 0.51899\n",
            "EPOCH 797\n",
            " Train loss: 0.51877\n",
            "EPOCH 798\n",
            " Train loss: 0.52154\n",
            "EPOCH 799\n",
            " Train loss: 0.52156\n",
            "EPOCH 800\n",
            " Train loss: 0.51839\n",
            "EPOCH 801\n",
            " Train loss: 0.52524\n",
            "EPOCH 802\n",
            " Train loss: 0.52588\n",
            "EPOCH 803\n",
            " Train loss: 0.51976\n",
            "EPOCH 804\n",
            " Train loss: 0.51872\n",
            "EPOCH 805\n",
            " Train loss: 0.51754\n",
            "EPOCH 806\n",
            " Train loss: 0.51916\n",
            "EPOCH 807\n",
            " Train loss: 0.51872\n",
            "EPOCH 808\n",
            " Train loss: 0.51823\n",
            "EPOCH 809\n",
            " Train loss: 0.53049\n",
            "EPOCH 810\n",
            " Train loss: 0.52416\n",
            "EPOCH 811\n",
            " Train loss: 0.52137\n",
            "EPOCH 812\n",
            " Train loss: 0.51728\n",
            "EPOCH 813\n",
            " Train loss: 0.51978\n",
            "EPOCH 814\n",
            " Train loss: 0.51703\n",
            "EPOCH 815\n",
            " Train loss: 0.51674\n",
            "EPOCH 816\n",
            " Train loss: 0.52113\n",
            "EPOCH 817\n",
            " Train loss: 0.52167\n",
            "EPOCH 818\n",
            " Train loss: 0.51453\n",
            "EPOCH 819\n",
            " Train loss: 0.51844\n",
            "EPOCH 820\n",
            " Train loss: 0.51776\n",
            "EPOCH 821\n",
            " Train loss: 0.51729\n",
            "EPOCH 822\n",
            " Train loss: 0.51753\n",
            "EPOCH 823\n",
            " Train loss: 0.51683\n",
            "EPOCH 824\n",
            " Train loss: 0.51719\n",
            "EPOCH 825\n",
            " Train loss: 0.51667\n",
            "EPOCH 826\n",
            " Train loss: 0.51729\n",
            "EPOCH 827\n",
            " Train loss: 0.51777\n",
            "EPOCH 828\n",
            " Train loss: 0.51590\n",
            "EPOCH 829\n",
            " Train loss: 0.51460\n",
            "EPOCH 830\n",
            " Train loss: 0.51697\n",
            "EPOCH 831\n",
            " Train loss: 0.52422\n",
            "EPOCH 832\n",
            " Train loss: 0.51937\n",
            "EPOCH 833\n",
            " Train loss: 0.51889\n",
            "EPOCH 834\n",
            " Train loss: 0.51577\n",
            "EPOCH 835\n",
            " Train loss: 0.51483\n",
            "EPOCH 836\n",
            " Train loss: 0.52239\n",
            "EPOCH 837\n",
            " Train loss: 0.51630\n",
            "EPOCH 838\n",
            " Train loss: 0.52225\n",
            "EPOCH 839\n",
            " Train loss: 0.52320\n",
            "EPOCH 840\n",
            " Train loss: 0.51853\n",
            "EPOCH 841\n",
            " Train loss: 0.51554\n",
            "EPOCH 842\n",
            " Train loss: 0.51627\n",
            "EPOCH 843\n",
            " Train loss: 0.51898\n",
            "EPOCH 844\n",
            " Train loss: 0.51503\n",
            "EPOCH 845\n",
            " Train loss: 0.51613\n",
            "EPOCH 846\n",
            " Train loss: 0.51837\n",
            "EPOCH 847\n",
            " Train loss: 0.51826\n",
            "EPOCH 848\n",
            " Train loss: 0.51785\n",
            "EPOCH 849\n",
            " Train loss: 0.52106\n",
            "EPOCH 850\n",
            " Train loss: 0.51731\n",
            "EPOCH 851\n",
            " Train loss: 0.52076\n",
            "EPOCH 852\n",
            " Train loss: 0.51903\n",
            "EPOCH 853\n",
            " Train loss: 0.52304\n",
            "EPOCH 854\n",
            " Train loss: 0.52844\n",
            "EPOCH 855\n",
            " Train loss: 0.51474\n",
            "EPOCH 856\n",
            " Train loss: 0.51432\n",
            "EPOCH 857\n",
            " Train loss: 0.52342\n",
            "EPOCH 858\n",
            " Train loss: 0.51437\n",
            "EPOCH 859\n",
            " Train loss: 0.51618\n",
            "EPOCH 860\n",
            " Train loss: 0.51284\n",
            "EPOCH 861\n",
            " Train loss: 0.51453\n",
            "EPOCH 862\n",
            " Train loss: 0.51610\n",
            "EPOCH 863\n",
            " Train loss: 0.51403\n",
            "EPOCH 864\n",
            " Train loss: 0.51357\n",
            "EPOCH 865\n",
            " Train loss: 0.51260\n",
            "EPOCH 866\n",
            " Train loss: 0.51469\n",
            "EPOCH 867\n",
            " Train loss: 0.51487\n",
            "EPOCH 868\n",
            " Train loss: 0.51488\n",
            "EPOCH 869\n",
            " Train loss: 0.51399\n",
            "EPOCH 870\n",
            " Train loss: 0.51284\n",
            "EPOCH 871\n",
            " Train loss: 0.51274\n",
            "EPOCH 872\n",
            " Train loss: 0.51344\n",
            "EPOCH 873\n",
            " Train loss: 0.51764\n",
            "EPOCH 874\n",
            " Train loss: 0.51107\n",
            "EPOCH 875\n",
            " Train loss: 0.51504\n",
            "EPOCH 876\n",
            " Train loss: 0.51466\n",
            "EPOCH 877\n",
            " Train loss: 0.51906\n",
            "EPOCH 878\n",
            " Train loss: 0.51636\n",
            "EPOCH 879\n",
            " Train loss: 0.51606\n",
            "EPOCH 880\n",
            " Train loss: 0.51644\n",
            "EPOCH 881\n",
            " Train loss: 0.51339\n",
            "EPOCH 882\n",
            " Train loss: 0.51630\n",
            "EPOCH 883\n",
            " Train loss: 0.51246\n",
            "EPOCH 884\n",
            " Train loss: 0.51405\n",
            "EPOCH 885\n",
            " Train loss: 0.51395\n",
            "EPOCH 886\n",
            " Train loss: 0.51447\n",
            "EPOCH 887\n",
            " Train loss: 0.51160\n",
            "EPOCH 888\n",
            " Train loss: 0.52230\n",
            "EPOCH 889\n",
            " Train loss: 0.51798\n",
            "EPOCH 890\n",
            " Train loss: 0.51563\n",
            "EPOCH 891\n",
            " Train loss: 0.51592\n",
            "EPOCH 892\n",
            " Train loss: 0.51139\n",
            "EPOCH 893\n",
            " Train loss: 0.51163\n",
            "EPOCH 894\n",
            " Train loss: 0.51530\n",
            "EPOCH 895\n",
            " Train loss: 0.51356\n",
            "EPOCH 896\n",
            " Train loss: 0.51396\n",
            "EPOCH 897\n",
            " Train loss: 0.51341\n",
            "EPOCH 898\n",
            " Train loss: 0.51296\n",
            "EPOCH 899\n",
            " Train loss: 0.51305\n",
            "EPOCH 900\n",
            " Train loss: 0.51429\n",
            "EPOCH 901\n",
            " Train loss: 0.51851\n",
            "EPOCH 902\n",
            " Train loss: 0.51310\n",
            "EPOCH 903\n",
            " Train loss: 0.51276\n",
            "EPOCH 904\n",
            " Train loss: 0.51067\n",
            "EPOCH 905\n",
            " Train loss: 0.51414\n",
            "EPOCH 906\n",
            " Train loss: 0.51326\n",
            "EPOCH 907\n",
            " Train loss: 0.51290\n",
            "EPOCH 908\n",
            " Train loss: 0.51077\n",
            "EPOCH 909\n",
            " Train loss: 0.51472\n",
            "EPOCH 910\n",
            " Train loss: 0.51644\n",
            "EPOCH 911\n",
            " Train loss: 0.51334\n",
            "EPOCH 912\n",
            " Train loss: 0.51454\n",
            "EPOCH 913\n",
            " Train loss: 0.51571\n",
            "EPOCH 914\n",
            " Train loss: 0.51620\n",
            "EPOCH 915\n",
            " Train loss: 0.50975\n",
            "EPOCH 916\n",
            " Train loss: 0.51241\n",
            "EPOCH 917\n",
            " Train loss: 0.51235\n",
            "EPOCH 918\n",
            " Train loss: 0.51586\n",
            "EPOCH 919\n",
            " Train loss: 0.51262\n",
            "EPOCH 920\n",
            " Train loss: 0.50945\n",
            "EPOCH 921\n",
            " Train loss: 0.51209\n",
            "EPOCH 922\n",
            " Train loss: 0.51112\n",
            "EPOCH 923\n",
            " Train loss: 0.51582\n",
            "EPOCH 924\n",
            " Train loss: 0.51330\n",
            "EPOCH 925\n",
            " Train loss: 0.50886\n",
            "EPOCH 926\n",
            " Train loss: 0.51065\n",
            "EPOCH 927\n",
            " Train loss: 0.51089\n",
            "EPOCH 928\n",
            " Train loss: 0.51370\n",
            "EPOCH 929\n",
            " Train loss: 0.51049\n",
            "EPOCH 930\n",
            " Train loss: 0.51316\n",
            "EPOCH 931\n",
            " Train loss: 0.50955\n",
            "EPOCH 932\n",
            " Train loss: 0.51016\n",
            "EPOCH 933\n",
            " Train loss: 0.51121\n",
            "EPOCH 934\n",
            " Train loss: 0.50780\n",
            "EPOCH 935\n",
            " Train loss: 0.51227\n",
            "EPOCH 936\n",
            " Train loss: 0.51203\n",
            "EPOCH 937\n",
            " Train loss: 0.51295\n",
            "EPOCH 938\n",
            " Train loss: 0.50948\n",
            "EPOCH 939\n",
            " Train loss: 0.51565\n",
            "EPOCH 940\n",
            " Train loss: 0.51101\n",
            "EPOCH 941\n",
            " Train loss: 0.51042\n",
            "EPOCH 942\n",
            " Train loss: 0.51483\n",
            "EPOCH 943\n",
            " Train loss: 0.51365\n",
            "EPOCH 944\n",
            " Train loss: 0.51803\n",
            "EPOCH 945\n",
            " Train loss: 0.51814\n",
            "EPOCH 946\n",
            " Train loss: 0.51807\n",
            "EPOCH 947\n",
            " Train loss: 0.51874\n",
            "EPOCH 948\n",
            " Train loss: 0.51052\n",
            "EPOCH 949\n",
            " Train loss: 0.50867\n",
            "EPOCH 950\n",
            " Train loss: 0.50951\n",
            "EPOCH 951\n",
            " Train loss: 0.50921\n",
            "EPOCH 952\n",
            " Train loss: 0.50947\n",
            "EPOCH 953\n",
            " Train loss: 0.51080\n",
            "EPOCH 954\n",
            " Train loss: 0.51852\n",
            "EPOCH 955\n",
            " Train loss: 0.50710\n",
            "EPOCH 956\n",
            " Train loss: 0.50924\n",
            "EPOCH 957\n",
            " Train loss: 0.50863\n",
            "EPOCH 958\n",
            " Train loss: 0.50979\n",
            "EPOCH 959\n",
            " Train loss: 0.51193\n",
            "EPOCH 960\n",
            " Train loss: 0.51171\n",
            "EPOCH 961\n",
            " Train loss: 0.50989\n",
            "EPOCH 962\n",
            " Train loss: 0.50761\n",
            "EPOCH 963\n",
            " Train loss: 0.51500\n",
            "EPOCH 964\n",
            " Train loss: 0.51207\n",
            "EPOCH 965\n",
            " Train loss: 0.50682\n",
            "EPOCH 966\n",
            " Train loss: 0.50926\n",
            "EPOCH 967\n",
            " Train loss: 0.51319\n",
            "EPOCH 968\n",
            " Train loss: 0.50875\n",
            "EPOCH 969\n",
            " Train loss: 0.50548\n",
            "EPOCH 970\n",
            " Train loss: 0.50725\n",
            "EPOCH 971\n",
            " Train loss: 0.50867\n",
            "EPOCH 972\n",
            " Train loss: 0.50859\n",
            "EPOCH 973\n",
            " Train loss: 0.51048\n",
            "EPOCH 974\n",
            " Train loss: 0.50975\n",
            "EPOCH 975\n",
            " Train loss: 0.50735\n",
            "EPOCH 976\n",
            " Train loss: 0.50840\n",
            "EPOCH 977\n",
            " Train loss: 0.51341\n",
            "EPOCH 978\n",
            " Train loss: 0.51124\n",
            "EPOCH 979\n",
            " Train loss: 0.51464\n",
            "EPOCH 980\n",
            " Train loss: 0.50710\n",
            "EPOCH 981\n",
            " Train loss: 0.51559\n",
            "EPOCH 982\n",
            " Train loss: 0.50920\n",
            "EPOCH 983\n",
            " Train loss: 0.50803\n",
            "EPOCH 984\n",
            " Train loss: 0.50991\n",
            "EPOCH 985\n",
            " Train loss: 0.51036\n",
            "EPOCH 986\n",
            " Train loss: 0.51425\n",
            "EPOCH 987\n",
            " Train loss: 0.51999\n",
            "EPOCH 988\n",
            " Train loss: 0.51758\n",
            "EPOCH 989\n",
            " Train loss: 0.51121\n",
            "EPOCH 990\n",
            " Train loss: 0.50655\n",
            "EPOCH 991\n",
            " Train loss: 0.50871\n",
            "EPOCH 992\n",
            " Train loss: 0.50650\n",
            "EPOCH 993\n",
            " Train loss: 0.50606\n",
            "EPOCH 994\n",
            " Train loss: 0.50831\n",
            "EPOCH 995\n",
            " Train loss: 0.50710\n",
            "EPOCH 996\n",
            " Train loss: 0.50696\n",
            "EPOCH 997\n",
            " Train loss: 0.50678\n",
            "EPOCH 998\n",
            " Train loss: 0.50612\n",
            "EPOCH 999\n",
            " Train loss: 0.50799\n"
          ]
        }
      ],
      "source": [
        "train_network(model, train_loader, criterion, optimizer, nepoch=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 447,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 56.827\n",
            "[1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
            " 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
            " 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
            " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0.\n",
            " 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "acc, true, pred = test_network(model, test_loader)\n",
        "print(pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
