{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6yUKoL5OMTZ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "diabetes_df = pd.read_csv(\"diabetic_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5j_Xsy6MTZ_",
        "outputId": "b30178c4-e135-4297-b2ee-9ada54722e52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8323\n",
            "2081\n",
            "15924\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class ReadmissionPredictionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, diabetes_df: pd.DataFrame, verbose = False):\n",
        "        # groups the readmitted patients because we want to focus on patients readmitted in 30 days\n",
        "        diabetes_df['readmitted'] = diabetes_df['readmitted'].replace({'<30': 'Yes', '>30': 'No'})\n",
        "        \n",
        "        # keeps only the last occurence of a patient\n",
        "        diabetes_df = diabetes_df.drop_duplicates(subset='patient_nbr', keep='last')\n",
        "        \n",
        "        # drop unnecessary columns\n",
        "        # patient_nbr, encounter_id are unecessary indexes\n",
        "        # medical_specialty, weight, payer_code has too many null values\n",
        "        # only focusing on initial diagnosis, so drop diag_1 and diag_2\n",
        "        # statistical analysis found that number_emergency, number_impatient, and number_outpatient are unecessary\n",
        "        columns_to_drop = ['patient_nbr', 'medical_specialty', 'weight', 'payer_code', 'encounter_id', 'diag_2', 'diag_3', 'number_emergency', 'number_inpatient', 'number_outpatient']\n",
        "        diabetes_df = diabetes_df.drop(columns=columns_to_drop)\n",
        "\n",
        "        # statistcal analysis found that all the medications besides insulin and metformin were unecessary\n",
        "        medications_to_drop = ['repaglinide', 'nateglinide', 'chlorpropamide', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',\n",
        "                   'glyburide-metformin', 'glipizide-metformin', 'metformin-pioglitazone', \n",
        "                   'metformin-rosiglitazone', 'glimepiride-pioglitazone', 'acetohexamide', 'citoglipton', 'examide', 'tolbutamide', 'glimepiride', 'glipizide', \n",
        "                    'glyburide', 'pioglitazone', 'rosiglitazone']\n",
        "        diabetes_df = diabetes_df.drop(columns=medications_to_drop)\n",
        "\n",
        "        # remove null values from remaining columns\n",
        "        for column in diabetes_df:\n",
        "            diabetes_df = diabetes_df[diabetes_df[column] != 'Unknown/Invalid']\n",
        "            diabetes_df = diabetes_df[diabetes_df[column] != '?']\n",
        "\n",
        "        # remove NA values from A1Cresult and max_glu_serum\n",
        "        # groups = diabetes_df.groupby('max_glu_serum')\n",
        "        # print(groups.size())\n",
        "\n",
        "        # unique_values = diabetes_df['max_glu_serum'].unique()\n",
        "        # print(unique_values)\n",
        "\n",
        "        diabetes_df.dropna(subset=['A1Cresult'], inplace=True)\n",
        "        #diabetes_df.dropna(subset=['max_glu_serum'], inplace=True)\n",
        "        diabetes_df.fillna('Norm', inplace=True)        \n",
        "\n",
        "        # max_glu_mapping = {\n",
        "        #     'Norm': 0,\n",
        "        #     '>200': 1,\n",
        "        #     '>300': 2\n",
        "        # }\n",
        "        # diabetes_df['max_glu_serum'] = diabetes_df['max_glu_serum'].replace(max_glu_mapping)\n",
        "\n",
        "        # unique_values = diabetes_df['max_glu_serum'].unique()\n",
        "        # print(unique_values)\n",
        "\n",
        "        #diabetes_df.dropna(subset=['max_glu_serum'], inplace=True)\n",
        "\n",
        "        # remove people that died since they won't be readmitted\n",
        "        values_to_remove = [11, 13, 14, 19, 20, 21] \n",
        "        diabetes_df = diabetes_df[~diabetes_df['discharge_disposition_id'].isin(values_to_remove)]\n",
        "            \n",
        "        # group similar discharge types into Home or Other\n",
        "        diabetes_df['discharge_disposition_id'] = diabetes_df['discharge_disposition_id'].apply(lambda x: 'Home' if x == 1 else 'Other')\n",
        "\n",
        "        # group similar admission types together\n",
        "        admission_mapping = {\n",
        "            2: 'Emergency',\n",
        "            1: 'Emergency',\n",
        "            7: 'Emergency',\n",
        "            6: 'Other',\n",
        "            5: 'Other',\n",
        "            8: 'Other',\n",
        "            3: 'Elective',\n",
        "            4: 'Newborn'\n",
        "        }\n",
        "        diabetes_df['admission_type_id'] = diabetes_df['admission_type_id'].replace(admission_mapping)\n",
        "\n",
        "        # group similar admission sources together\n",
        "        diabetes_df['admission_source_id'] = diabetes_df['admission_source_id'].map({\n",
        "            1: 'Physician Referral',\n",
        "            2: 'Physician Referral',\n",
        "            3: 'Physician Referral',\n",
        "            4: 'Other',\n",
        "            5: 'Other',\n",
        "            6: 'Other',\n",
        "            7: 'Emergency Room',\n",
        "            8: 'Other',\n",
        "            9: 'Other',\n",
        "            10: 'Other',\n",
        "            11: 'Other',\n",
        "            12: 'Other',\n",
        "            13: 'Other',\n",
        "            14: 'Other',\n",
        "            15: 'Other',\n",
        "            17: 'Other',\n",
        "            18: 'Other',\n",
        "            19: 'Other',\n",
        "            20: 'Other',\n",
        "            21: 'Other',\n",
        "            22: 'Other',\n",
        "            23: 'Other',\n",
        "            24: 'Other',\n",
        "            25: 'Other',\n",
        "            26: 'Other'\n",
        "        })\n",
        "        diabetes_df['admission_source_id'] = diabetes_df['admission_source_id'].replace(admission_mapping)\n",
        "\n",
        "        # apply a mapping for similar diagnosis groups\n",
        "        diag_mapping = {\n",
        "            'Infectious': [str(i) for i in range(1, 140)],\n",
        "            'Neoplasms': [str(i) for i in range(140, 240)],\n",
        "            'Endocrine': [str(i) for i in range(240, 280)],\n",
        "            'Circulatory': [str(i) for i in range(390, 460)],\n",
        "            'Respiratory': [str(i) for i in range(460, 520)],\n",
        "            'Digestive': [str(i) for i in range(520, 580)],\n",
        "            'Musculoskeletal': [str(i) for i in range(710, 740)],\n",
        "            'Genitourinary': [str(i) for i in range(580, 630)],\n",
        "            'Nervous': [str(i) for i in range(320, 390)],\n",
        "            'Symptoms': [str(i) for i in range(780, 800)]\n",
        "        }\n",
        "        def map_to_group(code):\n",
        "            for group, code_range in diag_mapping.items():\n",
        "                if code in code_range:\n",
        "                    return group\n",
        "            return 'Other'\n",
        "        diabetes_df['diag_1'] = diabetes_df['diag_1'].apply(map_to_group)\n",
        "\n",
        "        # one hot encoding for categorical columns\n",
        "        columns_to_convert = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'metformin', 'insulin', 'diag_1', 'gender', 'race', 'max_glu_serum', 'A1Cresult']\n",
        "        for column in columns_to_convert:\n",
        "            diabetes_df = pd.get_dummies(diabetes_df, columns=[column], dtype='int')\n",
        "\n",
        "        # convert age groups to integer values\n",
        "        age_mapping = {\n",
        "            '[0-10)': 5,\n",
        "            '[10-20)': 15,\n",
        "            '[20-30)': 25,\n",
        "            '[30-40)': 35,\n",
        "            '[40-50)': 45,\n",
        "            '[50-60)': 55,\n",
        "            '[60-70)': 65,\n",
        "            '[70-80)': 75,\n",
        "            '[80-90)': 85,\n",
        "            '[90-100)': 95\n",
        "        }\n",
        "        diabetes_df['age'] = diabetes_df['age'].replace(age_mapping)\n",
        "\n",
        "        # convert binary categories to integer values\n",
        "        readmit_mapping = {\n",
        "            'Yes': 1,\n",
        "            'no': 0,\n",
        "            'No': 0,\n",
        "            'YES' : 1,\n",
        "            'Ch': 1,\n",
        "            'NO': 0\n",
        "        }\n",
        "        diabetes_df['readmitted'] = diabetes_df['readmitted'].replace(readmit_mapping)\n",
        "        diabetes_df['diabetesMed'] = diabetes_df['diabetesMed'].replace(readmit_mapping)\n",
        "        diabetes_df['change'] = diabetes_df['change'].replace(readmit_mapping)\n",
        "\n",
        "        # remove the outliers from the quantitative variables (remove outliers beyond 1.5 * IQR)\n",
        "        def outliers_remover(df, columns_to_process):\n",
        "            aa = []\n",
        "            for column_name in columns_to_process:\n",
        "                if pd.api.types.is_numeric_dtype(df[column_name]):\n",
        "                    column = df[column_name]\n",
        "                    q1 = column.quantile(0.25)\n",
        "                    q3 = column.quantile(0.75)\n",
        "                    iqr = q3 - q1\n",
        "                    upper = q3 + 1.5 * iqr\n",
        "                    lower = q1 - 1.5 * iqr\n",
        "                    outliers = (column > upper) | (column < lower)\n",
        "                    aa.extend(outliers[outliers].index)\n",
        "            df = df.drop(aa).reset_index(drop=True)\n",
        "            return df\n",
        "        columns_to_process = ['num_medications', 'num_procedures', 'num_lab_procedures', 'time_in_hospital']\n",
        "        diabetes_df = outliers_remover(diabetes_df, columns_to_process)\n",
        "\n",
        "        # Randomly oversample the readmitted group to even out the distribution\n",
        "        # minority_class = diabetes_df[diabetes_df['readmitted'] == 1]\n",
        "        # majority_class = diabetes_df[diabetes_df['readmitted'] == 0]\n",
        "        # print(len(majority_class))\n",
        "        # print(len(minority_class))\n",
        "        # if len(minority_class) < len(majority_class):\n",
        "        #     minority_class = resample(minority_class, replace=True, n_samples=len(majority_class))\n",
        "        # diabetes_df = pd.concat([majority_class, minority_class])\n",
        "        # print(len(majority_class))\n",
        "        # print(len(minority_class))\n",
        "\n",
        "        # Split into independent (x) and dependent (y) variables\n",
        "        data_x = diabetes_df.select_dtypes(include=[int, float]).drop('readmitted', axis=1)\n",
        "        data_y = diabetes_df['readmitted']\n",
        "        self.input = torch.tensor(data_x.values).type(torch.float32)\n",
        "        self.output = torch.tensor(data_y.values).type(torch.float32)\n",
        "\n",
        "        self.df = diabetes_df\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.input[idx], self.output[idx])\n",
        "\n",
        "\n",
        "X = ReadmissionPredictionDataset(diabetes_df)\n",
        "train_dataset, test_dataset = train_test_split(X, test_size=.2, random_state=42)\n",
        "\n",
        "X_train, y_train = zip(*train_dataset)\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "\n",
        "oversampler = RandomOverSampler(sampling_strategy='minority')\n",
        "\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "oversampled_train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train_resampled), torch.tensor(y_train_resampled))\n",
        "train_loader = torch.utils.data.DataLoader(oversampled_train_dataset, batch_size=350, shuffle=True)\n",
        "\n",
        "print(len(oversampled_train_dataset))\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 350, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 350, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JUJcabOYMTaA",
        "outputId": "f956a61c-de7f-4de3-b241-30500f66afc6"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.layers = nn.Sequential(nn.Linear(49,16), nn.Sigmoid(), nn.Linear(16,32), nn.Sigmoid(), nn.Linear(32, 64), nn.Sigmoid(), nn.Linear(64, 1), nn.Sigmoid())\n",
        "    \n",
        "    def forward(self, x):\n",
        "        yhat = self.layers(x)\n",
        "        return yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_network(model, train_loader, criterion, optimizer, nepoch=100):\n",
        "    try:\n",
        "        for epoch in tqdm(range(nepoch)):\n",
        "            print('EPOCH %d'%epoch)\n",
        "            total_loss = 0\n",
        "            count = 0\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.squeeze(), labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                count += 1\n",
        "            print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting from training early')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_network(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    true, pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels  in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.round(outputs).squeeze()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            true.append(labels)\n",
        "            pred.append(predicted)\n",
        "    acc = (100 * correct / total)\n",
        "    print('accuracy: %0.3f' % (acc))\n",
        "    true = np.concatenate(true)\n",
        "    pred = np.concatenate(pred)\n",
        "    return acc, true, pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SimpleNet()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bceea4f40bc4cee8b84cb9714217379",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0\n",
            " Train loss: 0.69703\n",
            "EPOCH 1\n",
            " Train loss: 0.67573\n",
            "EPOCH 2\n",
            " Train loss: 0.64713\n",
            "EPOCH 3\n",
            " Train loss: 0.64179\n",
            "EPOCH 4\n",
            " Train loss: 0.63374\n",
            "EPOCH 5\n",
            " Train loss: 0.62390\n",
            "EPOCH 6\n",
            " Train loss: 0.62196\n",
            "EPOCH 7\n",
            " Train loss: 0.61519\n",
            "EPOCH 8\n",
            " Train loss: 0.60032\n",
            "EPOCH 9\n",
            " Train loss: 0.58657\n",
            "EPOCH 10\n",
            " Train loss: 0.57519\n",
            "EPOCH 11\n",
            " Train loss: 0.56191\n",
            "EPOCH 12\n",
            " Train loss: 0.54593\n",
            "EPOCH 13\n",
            " Train loss: 0.53762\n",
            "EPOCH 14\n",
            " Train loss: 0.52703\n",
            "EPOCH 15\n",
            " Train loss: 0.52794\n",
            "EPOCH 16\n",
            " Train loss: 0.51387\n",
            "EPOCH 17\n",
            " Train loss: 0.51493\n",
            "EPOCH 18\n",
            " Train loss: 0.50116\n",
            "EPOCH 19\n",
            " Train loss: 0.49069\n",
            "EPOCH 20\n",
            " Train loss: 0.50081\n",
            "EPOCH 21\n",
            " Train loss: 0.48004\n",
            "EPOCH 22\n",
            " Train loss: 0.48628\n",
            "EPOCH 23\n",
            " Train loss: 0.48785\n",
            "EPOCH 24\n",
            " Train loss: 0.46890\n",
            "EPOCH 25\n",
            " Train loss: 0.46337\n",
            "EPOCH 26\n",
            " Train loss: 0.45755\n",
            "EPOCH 27\n",
            " Train loss: 0.45713\n",
            "EPOCH 28\n",
            " Train loss: 0.46842\n",
            "EPOCH 29\n",
            " Train loss: 0.45177\n",
            "EPOCH 30\n",
            " Train loss: 0.45856\n",
            "EPOCH 31\n",
            " Train loss: 0.45155\n",
            "EPOCH 32\n",
            " Train loss: 0.44400\n",
            "EPOCH 33\n",
            " Train loss: 0.46393\n",
            "EPOCH 34\n",
            " Train loss: 0.44488\n",
            "EPOCH 35\n",
            " Train loss: 0.44647\n",
            "EPOCH 36\n",
            " Train loss: 0.43156\n",
            "EPOCH 37\n",
            " Train loss: 0.43294\n",
            "EPOCH 38\n",
            " Train loss: 0.42488\n",
            "EPOCH 39\n",
            " Train loss: 0.41164\n",
            "EPOCH 40\n",
            " Train loss: 0.41991\n",
            "EPOCH 41\n",
            " Train loss: 0.41324\n",
            "EPOCH 42\n",
            " Train loss: 0.41474\n",
            "EPOCH 43\n",
            " Train loss: 0.40404\n",
            "EPOCH 44\n",
            " Train loss: 0.41100\n",
            "EPOCH 45\n",
            " Train loss: 0.41352\n",
            "EPOCH 46\n",
            " Train loss: 0.39931\n",
            "EPOCH 47\n",
            " Train loss: 0.40703\n",
            "EPOCH 48\n",
            " Train loss: 0.38944\n",
            "EPOCH 49\n",
            " Train loss: 0.38927\n",
            "EPOCH 50\n",
            " Train loss: 0.40585\n",
            "EPOCH 51\n",
            " Train loss: 0.40179\n",
            "EPOCH 52\n",
            " Train loss: 0.40456\n",
            "EPOCH 53\n",
            " Train loss: 0.39864\n",
            "EPOCH 54\n",
            " Train loss: 0.41917\n",
            "EPOCH 55\n",
            " Train loss: 0.42291\n",
            "EPOCH 56\n",
            " Train loss: 0.41259\n",
            "EPOCH 57\n",
            " Train loss: 0.40316\n",
            "EPOCH 58\n",
            " Train loss: 0.42613\n",
            "EPOCH 59\n",
            " Train loss: 0.39652\n",
            "EPOCH 60\n",
            " Train loss: 0.37863\n",
            "EPOCH 61\n",
            " Train loss: 0.39078\n",
            "EPOCH 62\n",
            " Train loss: 0.41344\n",
            "EPOCH 63\n",
            " Train loss: 0.40649\n",
            "EPOCH 64\n",
            " Train loss: 0.38410\n",
            "EPOCH 65\n",
            " Train loss: 0.35914\n",
            "EPOCH 66\n",
            " Train loss: 0.36652\n",
            "EPOCH 67\n",
            " Train loss: 0.40478\n",
            "EPOCH 68\n",
            " Train loss: 0.40299\n",
            "EPOCH 69\n",
            " Train loss: 0.36920\n",
            "EPOCH 70\n",
            " Train loss: 0.39812\n",
            "EPOCH 71\n",
            " Train loss: 0.37883\n",
            "EPOCH 72\n",
            " Train loss: 0.37781\n",
            "EPOCH 73\n",
            " Train loss: 0.42190\n",
            "EPOCH 74\n",
            " Train loss: 0.39746\n",
            "EPOCH 75\n",
            " Train loss: 0.38777\n",
            "EPOCH 76\n",
            " Train loss: 0.34667\n",
            "EPOCH 77\n",
            " Train loss: 0.38942\n",
            "EPOCH 78\n",
            " Train loss: 0.42469\n",
            "EPOCH 79\n",
            " Train loss: 0.39306\n",
            "EPOCH 80\n",
            " Train loss: 0.35593\n",
            "EPOCH 81\n",
            " Train loss: 0.35283\n",
            "EPOCH 82\n",
            " Train loss: 0.43940\n",
            "EPOCH 83\n",
            " Train loss: 0.37779\n",
            "EPOCH 84\n",
            " Train loss: 0.37937\n",
            "EPOCH 85\n",
            " Train loss: 0.39392\n",
            "EPOCH 86\n",
            " Train loss: 0.37974\n",
            "EPOCH 87\n",
            " Train loss: 0.38950\n",
            "EPOCH 88\n",
            " Train loss: 0.34905\n",
            "EPOCH 89\n",
            " Train loss: 0.32963\n",
            "EPOCH 90\n",
            " Train loss: 0.35877\n",
            "EPOCH 91\n",
            " Train loss: 0.35295\n",
            "EPOCH 92\n",
            " Train loss: 0.33928\n",
            "EPOCH 93\n",
            " Train loss: 0.39202\n",
            "EPOCH 94\n",
            " Train loss: 0.38385\n",
            "EPOCH 95\n",
            " Train loss: 0.36665\n",
            "EPOCH 96\n",
            " Train loss: 0.36454\n",
            "EPOCH 97\n",
            " Train loss: 0.34515\n",
            "EPOCH 98\n",
            " Train loss: 0.32343\n",
            "EPOCH 99\n",
            " Train loss: 0.36900\n",
            "EPOCH 100\n",
            " Train loss: 0.39164\n",
            "EPOCH 101\n",
            " Train loss: 0.35853\n",
            "EPOCH 102\n",
            " Train loss: 0.37165\n",
            "EPOCH 103\n",
            " Train loss: 0.34776\n",
            "EPOCH 104\n",
            " Train loss: 0.37424\n",
            "EPOCH 105\n",
            " Train loss: 0.37849\n",
            "EPOCH 106\n",
            " Train loss: 0.32430\n",
            "EPOCH 107\n",
            " Train loss: 0.32252\n",
            "EPOCH 108\n",
            " Train loss: 0.40464\n",
            "EPOCH 109\n",
            " Train loss: 0.35637\n",
            "EPOCH 110\n",
            " Train loss: 0.33677\n",
            "EPOCH 111\n",
            " Train loss: 0.40914\n",
            "EPOCH 112\n",
            " Train loss: 0.38015\n",
            "EPOCH 113\n",
            " Train loss: 0.35166\n",
            "EPOCH 114\n",
            " Train loss: 0.41114\n",
            "EPOCH 115\n",
            " Train loss: 0.37118\n",
            "EPOCH 116\n",
            " Train loss: 0.38915\n",
            "EPOCH 117\n",
            " Train loss: 0.37656\n",
            "EPOCH 118\n",
            " Train loss: 0.42233\n",
            "EPOCH 119\n",
            " Train loss: 0.43190\n",
            "EPOCH 120\n",
            " Train loss: 0.37413\n",
            "EPOCH 121\n",
            " Train loss: 0.34652\n",
            "EPOCH 122\n",
            " Train loss: 0.35764\n",
            "EPOCH 123\n",
            " Train loss: 0.35734\n",
            "EPOCH 124\n",
            " Train loss: 0.34438\n",
            "EPOCH 125\n",
            " Train loss: 0.35182\n",
            "EPOCH 126\n",
            " Train loss: 0.41172\n",
            "EPOCH 127\n",
            " Train loss: 0.38424\n",
            "EPOCH 128\n",
            " Train loss: 0.37939\n",
            "EPOCH 129\n",
            " Train loss: 0.37843\n",
            "EPOCH 130\n",
            " Train loss: 0.37138\n",
            "EPOCH 131\n",
            " Train loss: 0.36852\n",
            "EPOCH 132\n",
            " Train loss: 0.33861\n",
            "EPOCH 133\n",
            " Train loss: 0.34922\n",
            "EPOCH 134\n",
            " Train loss: 0.41718\n",
            "EPOCH 135\n",
            " Train loss: 0.37494\n",
            "EPOCH 136\n",
            " Train loss: 0.36035\n",
            "EPOCH 137\n",
            " Train loss: 0.33263\n",
            "EPOCH 138\n",
            " Train loss: 0.36441\n",
            "EPOCH 139\n",
            " Train loss: 0.36980\n",
            "EPOCH 140\n",
            " Train loss: 0.34505\n",
            "EPOCH 141\n",
            " Train loss: 0.39686\n",
            "EPOCH 142\n",
            " Train loss: 0.34423\n",
            "EPOCH 143\n",
            " Train loss: 0.38291\n",
            "EPOCH 144\n",
            " Train loss: 0.38165\n",
            "EPOCH 145\n",
            " Train loss: 0.37188\n",
            "EPOCH 146\n",
            " Train loss: 0.32414\n",
            "EPOCH 147\n",
            " Train loss: 0.35458\n",
            "EPOCH 148\n",
            " Train loss: 0.35781\n",
            "EPOCH 149\n",
            " Train loss: 0.36782\n",
            "EPOCH 150\n",
            " Train loss: 0.36319\n",
            "EPOCH 151\n",
            " Train loss: 0.32917\n",
            "EPOCH 152\n",
            " Train loss: 0.32717\n",
            "EPOCH 153\n",
            " Train loss: 0.41326\n",
            "EPOCH 154\n",
            " Train loss: 0.48606\n",
            "EPOCH 155\n",
            " Train loss: 0.40457\n",
            "EPOCH 156\n",
            " Train loss: 0.36885\n",
            "EPOCH 157\n",
            " Train loss: 0.39259\n",
            "EPOCH 158\n",
            " Train loss: 0.35033\n",
            "EPOCH 159\n",
            " Train loss: 0.34485\n",
            "EPOCH 160\n",
            " Train loss: 0.33254\n",
            "EPOCH 161\n",
            " Train loss: 0.41818\n",
            "EPOCH 162\n",
            " Train loss: 0.36778\n",
            "EPOCH 163\n",
            " Train loss: 0.32556\n",
            "EPOCH 164\n",
            " Train loss: 0.31801\n",
            "EPOCH 165\n",
            " Train loss: 0.37884\n",
            "EPOCH 166\n",
            " Train loss: 0.35133\n",
            "EPOCH 167\n",
            " Train loss: 0.31410\n",
            "EPOCH 168\n",
            " Train loss: 0.42351\n",
            "EPOCH 169\n",
            " Train loss: 0.35067\n",
            "EPOCH 170\n",
            " Train loss: 0.34529\n",
            "EPOCH 171\n",
            " Train loss: 0.35066\n",
            "EPOCH 172\n",
            " Train loss: 0.38033\n",
            "EPOCH 173\n",
            " Train loss: 0.36281\n",
            "EPOCH 174\n",
            " Train loss: 0.35043\n",
            "EPOCH 175\n",
            " Train loss: 0.33520\n",
            "EPOCH 176\n",
            " Train loss: 0.36163\n",
            "EPOCH 177\n",
            " Train loss: 0.35369\n",
            "EPOCH 178\n",
            " Train loss: 0.31166\n",
            "EPOCH 179\n",
            " Train loss: 0.35388\n",
            "EPOCH 180\n",
            " Train loss: 0.37863\n",
            "EPOCH 181\n",
            " Train loss: 0.39235\n",
            "EPOCH 182\n",
            " Train loss: 0.33083\n",
            "EPOCH 183\n",
            " Train loss: 0.37468\n",
            "EPOCH 184\n",
            " Train loss: 0.32950\n",
            "EPOCH 185\n",
            " Train loss: 0.30704\n",
            "EPOCH 186\n",
            " Train loss: 0.33694\n",
            "EPOCH 187\n",
            " Train loss: 0.37602\n",
            "EPOCH 188\n",
            " Train loss: 0.31002\n",
            "EPOCH 189\n",
            " Train loss: 0.31073\n",
            "EPOCH 190\n",
            " Train loss: 0.33553\n",
            "EPOCH 191\n",
            " Train loss: 0.42550\n",
            "EPOCH 192\n",
            " Train loss: 0.37241\n",
            "EPOCH 193\n",
            " Train loss: 0.37305\n",
            "EPOCH 194\n",
            " Train loss: 0.31462\n",
            "EPOCH 195\n",
            " Train loss: 0.30431\n",
            "EPOCH 196\n",
            " Train loss: 0.40034\n",
            "EPOCH 197\n",
            " Train loss: 0.42662\n",
            "EPOCH 198\n",
            " Train loss: 0.33498\n",
            "EPOCH 199\n",
            " Train loss: 0.35332\n",
            "EPOCH 200\n",
            " Train loss: 0.34740\n",
            "EPOCH 201\n",
            " Train loss: 0.35263\n",
            "EPOCH 202\n",
            " Train loss: 0.33837\n",
            "EPOCH 203\n",
            " Train loss: 0.30964\n",
            "EPOCH 204\n",
            " Train loss: 0.34597\n",
            "EPOCH 205\n",
            " Train loss: 0.31883\n",
            "EPOCH 206\n",
            " Train loss: 0.33548\n",
            "EPOCH 207\n",
            " Train loss: 0.36471\n",
            "EPOCH 208\n",
            " Train loss: 0.34504\n",
            "EPOCH 209\n",
            " Train loss: 0.30774\n",
            "EPOCH 210\n",
            " Train loss: 0.29365\n",
            "EPOCH 211\n",
            " Train loss: 0.31682\n",
            "EPOCH 212\n",
            " Train loss: 0.37638\n",
            "EPOCH 213\n",
            " Train loss: 0.35910\n",
            "EPOCH 214\n",
            " Train loss: 0.40165\n",
            "EPOCH 215\n",
            " Train loss: 0.36718\n",
            "EPOCH 216\n",
            " Train loss: 0.35722\n",
            "EPOCH 217\n",
            " Train loss: 0.31554\n",
            "EPOCH 218\n",
            " Train loss: 0.29587\n",
            "EPOCH 219\n",
            " Train loss: 0.30610\n",
            "EPOCH 220\n",
            " Train loss: 0.33813\n",
            "EPOCH 221\n",
            " Train loss: 0.36646\n",
            "EPOCH 222\n",
            " Train loss: 0.43666\n",
            "EPOCH 223\n",
            " Train loss: 0.39003\n",
            "EPOCH 224\n",
            " Train loss: 0.37185\n",
            "EPOCH 225\n",
            " Train loss: 0.34280\n",
            "EPOCH 226\n",
            " Train loss: 0.34077\n",
            "EPOCH 227\n",
            " Train loss: 0.34248\n",
            "EPOCH 228\n",
            " Train loss: 0.30495\n",
            "EPOCH 229\n",
            " Train loss: 0.35818\n",
            "EPOCH 230\n",
            " Train loss: 0.45273\n",
            "EPOCH 231\n",
            " Train loss: 0.33147\n",
            "EPOCH 232\n",
            " Train loss: 0.30127\n",
            "EPOCH 233\n",
            " Train loss: 0.31332\n",
            "EPOCH 234\n",
            " Train loss: 0.32776\n",
            "EPOCH 235\n",
            " Train loss: 0.35715\n",
            "EPOCH 236\n",
            " Train loss: 0.35121\n",
            "EPOCH 237\n",
            " Train loss: 0.31228\n",
            "EPOCH 238\n",
            " Train loss: 0.29691\n",
            "EPOCH 239\n",
            " Train loss: 0.30844\n",
            "EPOCH 240\n",
            " Train loss: 0.36728\n",
            "EPOCH 241\n",
            " Train loss: 0.41873\n",
            "EPOCH 242\n",
            " Train loss: 0.33456\n",
            "EPOCH 243\n",
            " Train loss: 0.34870\n",
            "EPOCH 244\n",
            " Train loss: 0.31311\n",
            "EPOCH 245\n",
            " Train loss: 0.31294\n",
            "EPOCH 246\n",
            " Train loss: 0.29611\n",
            "EPOCH 247\n",
            " Train loss: 0.38810\n",
            "EPOCH 248\n",
            " Train loss: 0.34937\n",
            "EPOCH 249\n",
            " Train loss: 0.34228\n",
            "EPOCH 250\n",
            " Train loss: 0.33865\n",
            "EPOCH 251\n",
            " Train loss: 0.33948\n",
            "EPOCH 252\n",
            " Train loss: 0.34551\n",
            "EPOCH 253\n",
            " Train loss: 0.42230\n",
            "EPOCH 254\n",
            " Train loss: 0.50440\n",
            "EPOCH 255\n",
            " Train loss: 0.37945\n",
            "EPOCH 256\n",
            " Train loss: 0.37040\n",
            "EPOCH 257\n",
            " Train loss: 0.38969\n",
            "EPOCH 258\n",
            " Train loss: 0.35501\n",
            "EPOCH 259\n",
            " Train loss: 0.36861\n",
            "EPOCH 260\n",
            " Train loss: 0.34425\n",
            "EPOCH 261\n",
            " Train loss: 0.36257\n",
            "EPOCH 262\n",
            " Train loss: 0.32484\n",
            "EPOCH 263\n",
            " Train loss: 0.37314\n",
            "EPOCH 264\n",
            " Train loss: 0.37171\n",
            "EPOCH 265\n",
            " Train loss: 0.36746\n",
            "EPOCH 266\n",
            " Train loss: 0.37503\n",
            "EPOCH 267\n",
            " Train loss: 0.44918\n",
            "EPOCH 268\n",
            " Train loss: 0.42722\n",
            "EPOCH 269\n",
            " Train loss: 0.50244\n",
            "EPOCH 270\n",
            " Train loss: 0.42347\n",
            "EPOCH 271\n",
            " Train loss: 0.36676\n",
            "EPOCH 272\n",
            " Train loss: 0.34166\n",
            "EPOCH 273\n",
            " Train loss: 0.32390\n",
            "EPOCH 274\n",
            " Train loss: 0.37690\n",
            "EPOCH 275\n",
            " Train loss: 0.40982\n",
            "EPOCH 276\n",
            " Train loss: 0.36030\n",
            "EPOCH 277\n",
            " Train loss: 0.38914\n",
            "EPOCH 278\n",
            " Train loss: 0.37267\n",
            "EPOCH 279\n",
            " Train loss: 0.33712\n",
            "EPOCH 280\n",
            " Train loss: 0.32855\n",
            "EPOCH 281\n",
            " Train loss: 0.38351\n",
            "EPOCH 282\n",
            " Train loss: 0.35584\n",
            "EPOCH 283\n",
            " Train loss: 0.34566\n",
            "EPOCH 284\n",
            " Train loss: 0.32774\n",
            "EPOCH 285\n",
            " Train loss: 0.33583\n",
            "EPOCH 286\n",
            " Train loss: 0.35672\n",
            "EPOCH 287\n",
            " Train loss: 0.43172\n",
            "EPOCH 288\n",
            " Train loss: 0.42289\n",
            "EPOCH 289\n",
            " Train loss: 0.36070\n",
            "EPOCH 290\n",
            " Train loss: 0.34249\n",
            "EPOCH 291\n",
            " Train loss: 0.33685\n",
            "EPOCH 292\n",
            " Train loss: 0.39262\n",
            "EPOCH 293\n",
            " Train loss: 0.39041\n",
            "EPOCH 294\n",
            " Train loss: 0.33875\n",
            "EPOCH 295\n",
            " Train loss: 0.31941\n",
            "EPOCH 296\n",
            " Train loss: 0.37081\n",
            "EPOCH 297\n",
            " Train loss: 0.32177\n",
            "EPOCH 298\n",
            " Train loss: 0.34704\n",
            "EPOCH 299\n",
            " Train loss: 0.33152\n",
            "EPOCH 300\n",
            " Train loss: 0.40021\n",
            "EPOCH 301\n",
            " Train loss: 0.32143\n",
            "EPOCH 302\n",
            " Train loss: 0.33135\n",
            "EPOCH 303\n",
            " Train loss: 0.37210\n",
            "EPOCH 304\n",
            " Train loss: 0.33027\n",
            "EPOCH 305\n",
            " Train loss: 0.33819\n",
            "EPOCH 306\n",
            " Train loss: 0.37021\n",
            "EPOCH 307\n",
            " Train loss: 0.33719\n",
            "EPOCH 308\n",
            " Train loss: 0.32497\n",
            "EPOCH 309\n",
            " Train loss: 0.33415\n",
            "EPOCH 310\n",
            " Train loss: 0.36642\n",
            "EPOCH 311\n",
            " Train loss: 0.32675\n",
            "EPOCH 312\n",
            " Train loss: 0.36913\n",
            "EPOCH 313\n",
            " Train loss: 0.34734\n",
            "EPOCH 314\n",
            " Train loss: 0.33578\n",
            "EPOCH 315\n",
            " Train loss: 0.34722\n",
            "EPOCH 316\n",
            " Train loss: 0.38253\n",
            "EPOCH 317\n",
            " Train loss: 0.39384\n",
            "EPOCH 318\n",
            " Train loss: 0.32980\n",
            "EPOCH 319\n",
            " Train loss: 0.30453\n",
            "EPOCH 320\n",
            " Train loss: 0.35531\n",
            "EPOCH 321\n",
            " Train loss: 0.43450\n",
            "EPOCH 322\n",
            " Train loss: 0.41161\n",
            "EPOCH 323\n",
            " Train loss: 0.39503\n",
            "EPOCH 324\n",
            " Train loss: 0.39083\n",
            "EPOCH 325\n",
            " Train loss: 0.40474\n",
            "EPOCH 326\n",
            " Train loss: 0.44156\n",
            "EPOCH 327\n",
            " Train loss: 0.43018\n",
            "EPOCH 328\n",
            " Train loss: 0.42357\n",
            "EPOCH 329\n",
            " Train loss: 0.45532\n",
            "EPOCH 330\n",
            " Train loss: 0.39699\n",
            "EPOCH 331\n",
            " Train loss: 0.34721\n",
            "EPOCH 332\n",
            " Train loss: 0.32128\n",
            "EPOCH 333\n",
            " Train loss: 0.33473\n",
            "EPOCH 334\n",
            " Train loss: 0.32978\n",
            "EPOCH 335\n",
            " Train loss: 0.35404\n",
            "EPOCH 336\n",
            " Train loss: 0.32882\n",
            "EPOCH 337\n",
            " Train loss: 0.30968\n",
            "EPOCH 338\n",
            " Train loss: 0.35311\n",
            "EPOCH 339\n",
            " Train loss: 0.37189\n",
            "EPOCH 340\n",
            " Train loss: 0.32652\n",
            "EPOCH 341\n",
            " Train loss: 0.33279\n",
            "EPOCH 342\n",
            " Train loss: 0.31774\n",
            "EPOCH 343\n",
            " Train loss: 0.35450\n",
            "EPOCH 344\n",
            " Train loss: 0.37639\n",
            "EPOCH 345\n",
            " Train loss: 0.33192\n",
            "EPOCH 346\n",
            " Train loss: 0.32339\n",
            "EPOCH 347\n",
            " Train loss: 0.30381\n",
            "EPOCH 348\n",
            " Train loss: 0.30474\n",
            "EPOCH 349\n",
            " Train loss: 0.36208\n",
            "EPOCH 350\n",
            " Train loss: 0.49078\n",
            "EPOCH 351\n",
            " Train loss: 0.35104\n",
            "EPOCH 352\n",
            " Train loss: 0.30432\n",
            "EPOCH 353\n",
            " Train loss: 0.32976\n",
            "EPOCH 354\n",
            " Train loss: 0.31566\n",
            "EPOCH 355\n",
            " Train loss: 0.33152\n",
            "EPOCH 356\n",
            " Train loss: 0.32925\n",
            "EPOCH 357\n",
            " Train loss: 0.32852\n",
            "EPOCH 358\n",
            " Train loss: 0.30846\n",
            "EPOCH 359\n",
            " Train loss: 0.29910\n",
            "EPOCH 360\n",
            " Train loss: 0.34558\n",
            "EPOCH 361\n",
            " Train loss: 0.44484\n",
            "EPOCH 362\n",
            " Train loss: 0.39405\n",
            "EPOCH 363\n",
            " Train loss: 0.45473\n",
            "EPOCH 364\n",
            " Train loss: 0.41554\n",
            "EPOCH 365\n",
            " Train loss: 0.35964\n",
            "EPOCH 366\n",
            " Train loss: 0.31153\n",
            "EPOCH 367\n",
            " Train loss: 0.31630\n",
            "EPOCH 368\n",
            " Train loss: 0.32889\n",
            "EPOCH 369\n",
            " Train loss: 0.29421\n",
            "EPOCH 370\n",
            " Train loss: 0.29438\n",
            "EPOCH 371\n",
            " Train loss: 0.31527\n",
            "EPOCH 372\n",
            " Train loss: 0.31428\n",
            "EPOCH 373\n",
            " Train loss: 0.33396\n",
            "EPOCH 374\n",
            " Train loss: 0.35185\n",
            "EPOCH 375\n",
            " Train loss: 0.31416\n",
            "EPOCH 376\n",
            " Train loss: 0.31232\n",
            "EPOCH 377\n",
            " Train loss: 0.29536\n",
            "EPOCH 378\n",
            " Train loss: 0.30498\n",
            "EPOCH 379\n",
            " Train loss: 0.29931\n",
            "EPOCH 380\n",
            " Train loss: 0.30186\n",
            "EPOCH 381\n",
            " Train loss: 0.30094\n",
            "EPOCH 382\n",
            " Train loss: 0.39969\n",
            "EPOCH 383\n",
            " Train loss: 0.36514\n",
            "EPOCH 384\n",
            " Train loss: 0.35124\n",
            "EPOCH 385\n",
            " Train loss: 0.35593\n",
            "EPOCH 386\n",
            " Train loss: 0.35487\n",
            "EPOCH 387\n",
            " Train loss: 0.31382\n",
            "EPOCH 388\n",
            " Train loss: 0.31887\n",
            "EPOCH 389\n",
            " Train loss: 0.36044\n",
            "EPOCH 390\n",
            " Train loss: 0.36793\n",
            "EPOCH 391\n",
            " Train loss: 0.37277\n",
            "EPOCH 392\n",
            " Train loss: 0.34963\n",
            "EPOCH 393\n",
            " Train loss: 0.29490\n",
            "EPOCH 394\n",
            " Train loss: 0.30426\n",
            "EPOCH 395\n",
            " Train loss: 0.31499\n",
            "EPOCH 396\n",
            " Train loss: 0.37594\n",
            "EPOCH 397\n",
            " Train loss: 0.35741\n",
            "EPOCH 398\n",
            " Train loss: 0.34875\n",
            "EPOCH 399\n",
            " Train loss: 0.35163\n",
            "EPOCH 400\n",
            " Train loss: 0.32387\n",
            "EPOCH 401\n",
            " Train loss: 0.29793\n",
            "EPOCH 402\n",
            " Train loss: 0.33070\n",
            "EPOCH 403\n",
            " Train loss: 0.37632\n",
            "EPOCH 404\n",
            " Train loss: 0.40063\n",
            "EPOCH 405\n",
            " Train loss: 0.36903\n",
            "EPOCH 406\n",
            " Train loss: 0.38125\n",
            "EPOCH 407\n",
            " Train loss: 0.35715\n",
            "EPOCH 408\n",
            " Train loss: 0.31465\n",
            "EPOCH 409\n",
            " Train loss: 0.30751\n",
            "EPOCH 410\n",
            " Train loss: 0.31574\n",
            "EPOCH 411\n",
            " Train loss: 0.30680\n",
            "EPOCH 412\n",
            " Train loss: 0.30908\n",
            "EPOCH 413\n",
            " Train loss: 0.33607\n",
            "EPOCH 414\n",
            " Train loss: 0.30152\n",
            "EPOCH 415\n",
            " Train loss: 0.28949\n",
            "EPOCH 416\n",
            " Train loss: 0.33402\n",
            "EPOCH 417\n",
            " Train loss: 0.31392\n",
            "EPOCH 418\n",
            " Train loss: 0.38818\n",
            "EPOCH 419\n",
            " Train loss: 0.35581\n",
            "EPOCH 420\n",
            " Train loss: 0.32682\n",
            "EPOCH 421\n",
            " Train loss: 0.36372\n",
            "EPOCH 422\n",
            " Train loss: 0.36791\n",
            "EPOCH 423\n",
            " Train loss: 0.36374\n",
            "EPOCH 424\n",
            " Train loss: 0.36604\n",
            "EPOCH 425\n",
            " Train loss: 0.36386\n",
            "EPOCH 426\n",
            " Train loss: 0.35806\n",
            "EPOCH 427\n",
            " Train loss: 0.35583\n",
            "EPOCH 428\n",
            " Train loss: 0.34803\n",
            "EPOCH 429\n",
            " Train loss: 0.32276\n",
            "EPOCH 430\n",
            " Train loss: 0.33540\n",
            "EPOCH 431\n",
            " Train loss: 0.33913\n",
            "EPOCH 432\n",
            " Train loss: 0.33187\n",
            "EPOCH 433\n",
            " Train loss: 0.33722\n",
            "EPOCH 434\n",
            " Train loss: 0.30173\n",
            "EPOCH 435\n",
            " Train loss: 0.33589\n",
            "EPOCH 436\n",
            " Train loss: 0.34048\n",
            "EPOCH 437\n",
            " Train loss: 0.29246\n",
            "EPOCH 438\n",
            " Train loss: 0.29623\n",
            "EPOCH 439\n",
            " Train loss: 0.31543\n",
            "EPOCH 440\n",
            " Train loss: 0.30509\n",
            "EPOCH 441\n",
            " Train loss: 0.30734\n",
            "EPOCH 442\n",
            " Train loss: 0.41845\n",
            "EPOCH 443\n",
            " Train loss: 0.38772\n",
            "EPOCH 444\n",
            " Train loss: 0.29223\n",
            "EPOCH 445\n",
            " Train loss: 0.33839\n",
            "EPOCH 446\n",
            " Train loss: 0.31471\n",
            "EPOCH 447\n",
            " Train loss: 0.33371\n",
            "EPOCH 448\n",
            " Train loss: 0.38831\n",
            "EPOCH 449\n",
            " Train loss: 0.35008\n",
            "EPOCH 450\n",
            " Train loss: 0.35983\n",
            "EPOCH 451\n",
            " Train loss: 0.35193\n",
            "EPOCH 452\n",
            " Train loss: 0.40936\n",
            "EPOCH 453\n",
            " Train loss: 0.35229\n",
            "EPOCH 454\n",
            " Train loss: 0.34161\n",
            "EPOCH 455\n",
            " Train loss: 0.36129\n",
            "EPOCH 456\n",
            " Train loss: 0.39485\n",
            "EPOCH 457\n",
            " Train loss: 0.30406\n",
            "EPOCH 458\n",
            " Train loss: 0.31380\n",
            "EPOCH 459\n",
            " Train loss: 0.30403\n",
            "EPOCH 460\n",
            " Train loss: 0.37040\n",
            "EPOCH 461\n",
            " Train loss: 0.38104\n",
            "EPOCH 462\n",
            " Train loss: 0.35680\n",
            "EPOCH 463\n",
            " Train loss: 0.37122\n",
            "EPOCH 464\n",
            " Train loss: 0.42570\n",
            "EPOCH 465\n",
            " Train loss: 0.37861\n",
            "EPOCH 466\n",
            " Train loss: 0.37329\n",
            "EPOCH 467\n",
            " Train loss: 0.43051\n",
            "EPOCH 468\n",
            " Train loss: 0.42323\n",
            "EPOCH 469\n",
            " Train loss: 0.38338\n",
            "EPOCH 470\n",
            " Train loss: 0.40574\n",
            "EPOCH 471\n",
            " Train loss: 0.41830\n",
            "EPOCH 472\n",
            " Train loss: 0.50351\n",
            "EPOCH 473\n",
            " Train loss: 0.38064\n",
            "EPOCH 474\n",
            " Train loss: 0.31907\n",
            "EPOCH 475\n",
            " Train loss: 0.29030\n",
            "EPOCH 476\n",
            " Train loss: 0.35025\n",
            "EPOCH 477\n",
            " Train loss: 0.31136\n",
            "EPOCH 478\n",
            " Train loss: 0.30336\n",
            "EPOCH 479\n",
            " Train loss: 0.29983\n",
            "EPOCH 480\n",
            " Train loss: 0.35703\n",
            "EPOCH 481\n",
            " Train loss: 0.36444\n",
            "EPOCH 482\n",
            " Train loss: 0.35509\n",
            "EPOCH 483\n",
            " Train loss: 0.33015\n",
            "EPOCH 484\n",
            " Train loss: 0.31533\n",
            "EPOCH 485\n",
            " Train loss: 0.30856\n",
            "EPOCH 486\n",
            " Train loss: 0.29943\n",
            "EPOCH 487\n",
            " Train loss: 0.30228\n",
            "EPOCH 488\n",
            " Train loss: 0.32207\n",
            "EPOCH 489\n",
            " Train loss: 0.31093\n",
            "EPOCH 490\n",
            " Train loss: 0.32883\n",
            "EPOCH 491\n",
            " Train loss: 0.38425\n",
            "EPOCH 492\n",
            " Train loss: 0.40134\n",
            "EPOCH 493\n",
            " Train loss: 0.41126\n",
            "EPOCH 494\n",
            " Train loss: 0.35914\n",
            "EPOCH 495\n",
            " Train loss: 0.29356\n",
            "EPOCH 496\n",
            " Train loss: 0.28290\n",
            "EPOCH 497\n",
            " Train loss: 0.29101\n",
            "EPOCH 498\n",
            " Train loss: 0.32050\n",
            "EPOCH 499\n",
            " Train loss: 0.32307\n"
          ]
        }
      ],
      "source": [
        "train_network(model, train_loader, criterion, optimizer, nepoch=500)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 64.200\n",
            "[0. 0. 0. ... 0. 0. 1.]\n"
          ]
        }
      ],
      "source": [
        "acc, true, pred = test_network(model, test_loader)\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'my_model.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
