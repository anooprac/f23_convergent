{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "6yUKoL5OMTZ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "diabetes_df = pd.read_csv(\"diabetic_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5j_Xsy6MTZ_",
        "outputId": "b30178c4-e135-4297-b2ee-9ada54722e52"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ReadmissionPredictionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, diabetes_df: pd.DataFrame, verbose = False):\n",
        "        # groups the readmitted patients because we want to focus on patients readmitted in 30 days\n",
        "        diabetes_df['readmitted'] = diabetes_df['readmitted'].replace({'<30': 'Yes', '>30': 'No'})\n",
        "        \n",
        "        # keeps only the last occurence of a patient\n",
        "        diabetes_df = diabetes_df.drop_duplicates(subset='patient_nbr', keep='last')\n",
        "        \n",
        "        # drop unnecessary columns\n",
        "        # patient_nbr, encounter_id are unecessary indexes\n",
        "        # medical_specialty, weight, payer_code has too many null values\n",
        "        # only focusing on initial diagnosis, so drop diag_1 and diag_2\n",
        "        # statistical analysis found that number_emergency, number_impatient, and number_outpatient are unecessary\n",
        "        columns_to_drop = ['patient_nbr', 'medical_specialty', 'weight', 'payer_code', 'encounter_id', 'diag_2', 'diag_3', 'number_emergency', 'number_inpatient', 'number_outpatient', 'race']\n",
        "        diabetes_df = diabetes_df.drop(columns=columns_to_drop)\n",
        "\n",
        "        # statistcal analysis found that all the medications besides insulin and metformin were unecessary\n",
        "        medications_to_drop = ['repaglinide', 'nateglinide', 'chlorpropamide', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',\n",
        "                   'glyburide-metformin', 'glipizide-metformin', 'metformin-pioglitazone', \n",
        "                   'metformin-rosiglitazone', 'glimepiride-pioglitazone', 'acetohexamide', 'citoglipton', 'examide', 'tolbutamide', 'glimepiride', 'glipizide', \n",
        "                    'glyburide', 'pioglitazone', 'rosiglitazone']\n",
        "        diabetes_df = diabetes_df.drop(columns=medications_to_drop)\n",
        "\n",
        "        # remove null values from remaining columns\n",
        "        for column in diabetes_df:\n",
        "            diabetes_df = diabetes_df[diabetes_df[column] != 'Unknown/Invalid']\n",
        "            diabetes_df = diabetes_df[diabetes_df[column] != '?']\n",
        "\n",
        "        # remove NA values from A1Cresult and max_glu_serum\n",
        "        # groups = diabetes_df.groupby('max_glu_serum')\n",
        "        # print(groups.size())\n",
        "\n",
        "        # unique_values = diabetes_df['max_glu_serum'].unique()\n",
        "        # print(unique_values)\n",
        "\n",
        "        diabetes_df.dropna(subset=['A1Cresult'], inplace=True)\n",
        "        diabetes_df.dropna(subset=['max_glu_serum'], inplace=True)\n",
        "        #diabetes_df.fillna('Norm', inplace=True) \n",
        "        \n",
        "        # max_glu_mapping = {\n",
        "        #     'Norm': 0,\n",
        "        #     '>200': 1,\n",
        "#        #     '>300': 2\n",
        "        # }\n",
        "        # diabetes_df['max_glu_serum'] = diabetes_df['max_glu_serum'].replace(max_glu_mapping)\n",
        "\n",
        "        # unique_values = diabetes_df['max_glu_serum'].unique()\n",
        "        # print(unique_values)\n",
        "\n",
        "        #diabetes_df.dropna(subset=['max_glu_serum'], inplace=True)\n",
        "\n",
        "        # remove people that died since they won't be readmitted\n",
        "        values_to_remove = [11, 13, 14, 19, 20, 21] \n",
        "        diabetes_df = diabetes_df[~diabetes_df['discharge_disposition_id'].isin(values_to_remove)]\n",
        "            \n",
        "        # group similar discharge types into Home or Other\n",
        "        diabetes_df['discharge_disposition_id'] = diabetes_df['discharge_disposition_id'].apply(lambda x: 'Home' if x == 1 else 'Other')\n",
        "\n",
        "        # group similar admission types together\n",
        "        admission_mapping = {\n",
        "            2: 'Emergency',\n",
        "            1: 'Emergency',\n",
        "            7: 'Emergency',\n",
        "            6: 'Other',\n",
        "            5: 'Other',\n",
        "            8: 'Other',\n",
        "            3: 'Elective',\n",
        "            4: 'Newborn'\n",
        "        }\n",
        "        diabetes_df['admission_type_id'] = diabetes_df['admission_type_id'].replace(admission_mapping)\n",
        "\n",
        "        # group similar admission sources together\n",
        "        diabetes_df['admission_source_id'] = diabetes_df['admission_source_id'].map({\n",
        "            1: 'Physician Referral',\n",
        "            2: 'Physician Referral',\n",
        "            3: 'Physician Referral',\n",
        "            4: 'Other',\n",
        "            5: 'Other',\n",
        "            6: 'Other',\n",
        "            7: 'Emergency Room',\n",
        "            8: 'Other',\n",
        "            9: 'Other',\n",
        "            10: 'Other',\n",
        "            11: 'Other',\n",
        "            12: 'Other',\n",
        "            13: 'Other',\n",
        "            14: 'Other',\n",
        "            15: 'Other',\n",
        "            17: 'Other',\n",
        "            18: 'Other',\n",
        "            19: 'Other',\n",
        "            20: 'Other',\n",
        "            21: 'Other',\n",
        "            22: 'Other',\n",
        "            23: 'Other',\n",
        "            24: 'Other',\n",
        "            25: 'Other',\n",
        "            26: 'Other'\n",
        "        })\n",
        "        diabetes_df['admission_source_id'] = diabetes_df['admission_source_id'].replace(admission_mapping)\n",
        "\n",
        "        # apply a mapping for similar diagnosis groups\n",
        "        diag_mapping = {\n",
        "            'Infectious': [str(i) for i in range(1, 140)],\n",
        "            'Neoplasms': [str(i) for i in range(140, 240)],\n",
        "            'Endocrine': [str(i) for i in range(240, 280)],\n",
        "            'Circulatory': [str(i) for i in range(390, 460)],\n",
        "            'Respiratory': [str(i) for i in range(460, 520)],\n",
        "            'Digestive': [str(i) for i in range(520, 580)],\n",
        "            'Musculoskeletal': [str(i) for i in range(710, 740)],\n",
        "            'Genitourinary': [str(i) for i in range(580, 630)],\n",
        "            'Nervous': [str(i) for i in range(320, 390)],\n",
        "            'Symptoms': [str(i) for i in range(780, 800)]\n",
        "        }\n",
        "        def map_to_group(code):\n",
        "            for group, code_range in diag_mapping.items():\n",
        "                if code in code_range:\n",
        "                    return group\n",
        "            return 'Other'\n",
        "        diabetes_df['diag_1'] = diabetes_df['diag_1'].apply(map_to_group)\n",
        "\n",
        "        # one hot encoding for categorical columns\n",
        "        columns_to_convert = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'metformin', 'insulin', 'diag_1', 'gender', 'max_glu_serum', 'A1Cresult']\n",
        "        for column in columns_to_convert:\n",
        "            diabetes_df = pd.get_dummies(diabetes_df, columns=[column], dtype='int')\n",
        "\n",
        "        # convert age groups to integer values\n",
        "        age_mapping = {\n",
        "            '[0-10)': 5,\n",
        "            '[10-20)': 15,\n",
        "            '[20-30)': 25,\n",
        "            '[30-40)': 35,\n",
        "            '[40-50)': 45,\n",
        "            '[50-60)': 55,\n",
        "            '[60-70)': 65,\n",
        "            '[70-80)': 75,\n",
        "            '[80-90)': 85,\n",
        "            '[90-100)': 95\n",
        "        }\n",
        "        diabetes_df['age'] = diabetes_df['age'].replace(age_mapping)\n",
        "\n",
        "        # convert binary categories to integer values\n",
        "        readmit_mapping = {\n",
        "            'Yes': 1,\n",
        "            'no': 0,\n",
        "            'No': 0,\n",
        "            'YES' : 1,\n",
        "            'Ch': 1,\n",
        "            'NO': 0\n",
        "        }\n",
        "        diabetes_df['readmitted'] = diabetes_df['readmitted'].replace(readmit_mapping)\n",
        "        diabetes_df['diabetesMed'] = diabetes_df['diabetesMed'].replace(readmit_mapping)\n",
        "        diabetes_df['change'] = diabetes_df['change'].replace(readmit_mapping)\n",
        "\n",
        "        # remove the outliers from the quantitative variables (remove outliers beyond 1.5 * IQR)\n",
        "        def outliers_remover(df, columns_to_process):\n",
        "            aa = []\n",
        "            for column_name in columns_to_process:\n",
        "                if pd.api.types.is_numeric_dtype(df[column_name]):\n",
        "                    column = df[column_name]\n",
        "                    q1 = column.quantile(0.25)\n",
        "                    q3 = column.quantile(0.75)\n",
        "                    iqr = q3 - q1\n",
        "                    upper = q3 + 1.5 * iqr\n",
        "                    lower = q1 - 1.5 * iqr\n",
        "                    outliers = (column > upper) | (column < lower)\n",
        "                    aa.extend(outliers[outliers].index)\n",
        "            df = df.drop(aa).reset_index(drop=True)\n",
        "            return df\n",
        "        columns_to_process = ['num_medications', 'num_procedures', 'num_lab_procedures', 'time_in_hospital']\n",
        "        diabetes_df = outliers_remover(diabetes_df, columns_to_process)\n",
        "\n",
        "        # Randomly oversample the readmitted group to even out the distribution\n",
        "        minority_class = diabetes_df[diabetes_df['readmitted'] == 1]\n",
        "        majority_class = diabetes_df[diabetes_df['readmitted'] == 0]\n",
        "        if len(minority_class) < len(majority_class):\n",
        "            minority_class = resample(minority_class, replace=True, n_samples=len(majority_class))\n",
        "        diabetes_df = pd.concat([majority_class, minority_class])\n",
        "\n",
        "        # Split into independent (x) and dependent (y) variables\n",
        "        data_x = diabetes_df.select_dtypes(include=[int, float]).drop('readmitted', axis=1)\n",
        "        data_y = diabetes_df['readmitted']\n",
        "        self.input = torch.tensor(data_x.values).type(torch.float32)\n",
        "        self.output = torch.tensor(data_y.values).type(torch.float32)\n",
        "\n",
        "        self.df = diabetes_df\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.input[idx], self.output[idx])\n",
        "\n",
        "\n",
        "X = ReadmissionPredictionDataset(diabetes_df)\n",
        "train_dataset, test_dataset = train_test_split(X, test_size=.2, random_state=42)\n",
        "\n",
        "# X_train, y_train = zip(*train_dataset)\n",
        "# print(len(train_dataset))\n",
        "# print(len(test_dataset))\n",
        "\n",
        "# oversampler = RandomOverSampler(sampling_strategy='minority')\n",
        "\n",
        "# X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "# oversampled_train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train_resampled), torch.tensor(y_train_resampled))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=350, shuffle=True)\n",
        "\n",
        "# print(len(oversampled_train_dataset))\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 350, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 350, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# readmitted = X.df[X.df['readmitted'] == 1]\n",
        "# not_readmitted = X.df[X.df['readmitted'] == 0]\n",
        "\n",
        "# readmitted_counts = readmitted[['race_AfricanAmerican', 'race_Asian', 'race_Caucasian', 'race_Hispanic', 'race_Other']].sum().values\n",
        "# not_readmitted_counts = not_readmitted[['race_AfricanAmerican', 'race_Asian', 'race_Caucasian', 'race_Hispanic', 'race_Other']].sum().values\n",
        "\n",
        "# categories = ['race_AfricanAmerican', 'race_Asian', 'race_Caucasian', 'race_Hispanic', 'race_Other']\n",
        "# bar_width = 0.35\n",
        "# index = np.arange(len(categories))\n",
        "\n",
        "# # Plotting\n",
        "# plt.figure(figsize=(10, 3))\n",
        "\n",
        "# plt.bar(index - bar_width/2, readmitted_counts, bar_width, label='Readmitted', color='skyblue')\n",
        "# plt.bar(index + bar_width/2, not_readmitted_counts, bar_width, label='Not Readmitted', color='orange')\n",
        "\n",
        "# plt.xlabel('Max Glu Serum Category')\n",
        "# plt.ylabel('Count')\n",
        "# plt.title('Distribution of Max Glu Serum for Readmitted vs. Not Readmitted Patients')\n",
        "# plt.xticks(index, categories)\n",
        "# plt.legend(title='Patient Status')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "JUJcabOYMTaA",
        "outputId": "f956a61c-de7f-4de3-b241-30500f66afc6"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.layers = nn.Sequential(nn.Linear(42,16), nn.Sigmoid(), nn.Linear(16,32), nn.Sigmoid(), nn.Linear(32, 64), nn.Sigmoid(), nn.Linear(64, 1), nn.Sigmoid())\n",
        "    \n",
        "    def forward(self, x):\n",
        "        yhat = self.layers(x)\n",
        "        return yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_network(model, train_loader, criterion, optimizer, nepoch=100):\n",
        "    try:\n",
        "        for epoch in tqdm(range(nepoch)):\n",
        "            print('EPOCH %d'%epoch)\n",
        "            total_loss = 0\n",
        "            count = 0\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.squeeze(), labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                count += 1\n",
        "            print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting from training early')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_network(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    true, pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels  in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.round(outputs).squeeze()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            true.append(labels)\n",
        "            pred.append(predicted)\n",
        "    acc = (100 * correct / total)\n",
        "    print('accuracy: %0.3f' % (acc))\n",
        "    true = np.concatenate(true)\n",
        "    pred = np.concatenate(pred)\n",
        "    return acc, true, pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SimpleNet()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e18acb6e6fe5461c9585d0670d33069e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0\n",
            " Train loss: 0.70763\n",
            "EPOCH 1\n",
            " Train loss: 0.69552\n",
            "EPOCH 2\n",
            " Train loss: 0.70367\n",
            "EPOCH 3\n",
            " Train loss: 0.69796\n",
            "EPOCH 4\n",
            " Train loss: 0.69311\n",
            "EPOCH 5\n",
            " Train loss: 0.69475\n",
            "EPOCH 6\n",
            " Train loss: 0.69761\n",
            "EPOCH 7\n",
            " Train loss: 0.69699\n",
            "EPOCH 8\n",
            " Train loss: 0.69429\n",
            "EPOCH 9\n",
            " Train loss: 0.69273\n",
            "EPOCH 10\n",
            " Train loss: 0.69341\n",
            "EPOCH 11\n",
            " Train loss: 0.69475\n",
            "EPOCH 12\n",
            " Train loss: 0.69485\n",
            "EPOCH 13\n",
            " Train loss: 0.69363\n",
            "EPOCH 14\n",
            " Train loss: 0.69237\n",
            "EPOCH 15\n",
            " Train loss: 0.69205\n",
            "EPOCH 16\n",
            " Train loss: 0.69243\n",
            "EPOCH 17\n",
            " Train loss: 0.69259\n",
            "EPOCH 18\n",
            " Train loss: 0.69195\n",
            "EPOCH 19\n",
            " Train loss: 0.69091\n",
            "EPOCH 20\n",
            " Train loss: 0.69006\n",
            "EPOCH 21\n",
            " Train loss: 0.68930\n",
            "EPOCH 22\n",
            " Train loss: 0.68834\n",
            "EPOCH 23\n",
            " Train loss: 0.68714\n",
            "EPOCH 24\n",
            " Train loss: 0.68568\n",
            "EPOCH 25\n",
            " Train loss: 0.68369\n",
            "EPOCH 26\n",
            " Train loss: 0.68110\n",
            "EPOCH 27\n",
            " Train loss: 0.67782\n",
            "EPOCH 28\n",
            " Train loss: 0.67389\n",
            "EPOCH 29\n",
            " Train loss: 0.66949\n",
            "EPOCH 30\n",
            " Train loss: 0.66389\n",
            "EPOCH 31\n",
            " Train loss: 0.65684\n",
            "EPOCH 32\n",
            " Train loss: 0.64876\n",
            "EPOCH 33\n",
            " Train loss: 0.63928\n",
            "EPOCH 34\n",
            " Train loss: 0.62816\n",
            "EPOCH 35\n",
            " Train loss: 0.61623\n",
            "EPOCH 36\n",
            " Train loss: 0.60308\n",
            "EPOCH 37\n",
            " Train loss: 0.58992\n",
            "EPOCH 38\n",
            " Train loss: 0.57917\n",
            "EPOCH 39\n",
            " Train loss: 0.56382\n",
            "EPOCH 40\n",
            " Train loss: 0.54950\n",
            "EPOCH 41\n",
            " Train loss: 0.54078\n",
            "EPOCH 42\n",
            " Train loss: 0.53261\n",
            "EPOCH 43\n",
            " Train loss: 0.52086\n",
            "EPOCH 44\n",
            " Train loss: 0.50764\n",
            "EPOCH 45\n",
            " Train loss: 0.50387\n",
            "EPOCH 46\n",
            " Train loss: 0.49702\n",
            "EPOCH 47\n",
            " Train loss: 0.48066\n",
            "EPOCH 48\n",
            " Train loss: 0.47926\n",
            "EPOCH 49\n",
            " Train loss: 0.47263\n",
            "EPOCH 50\n",
            " Train loss: 0.45636\n",
            "EPOCH 51\n",
            " Train loss: 0.45905\n",
            "EPOCH 52\n",
            " Train loss: 0.44559\n",
            "EPOCH 53\n",
            " Train loss: 0.43646\n",
            "EPOCH 54\n",
            " Train loss: 0.43445\n",
            "EPOCH 55\n",
            " Train loss: 0.41708\n",
            "EPOCH 56\n",
            " Train loss: 0.41501\n",
            "EPOCH 57\n",
            " Train loss: 0.40334\n",
            "EPOCH 58\n",
            " Train loss: 0.39382\n",
            "EPOCH 59\n",
            " Train loss: 0.38958\n",
            "EPOCH 60\n",
            " Train loss: 0.37577\n",
            "EPOCH 61\n",
            " Train loss: 0.36978\n",
            "EPOCH 62\n",
            " Train loss: 0.35995\n",
            "EPOCH 63\n",
            " Train loss: 0.34723\n",
            "EPOCH 64\n",
            " Train loss: 0.33864\n",
            "EPOCH 65\n",
            " Train loss: 0.33398\n",
            "EPOCH 66\n",
            " Train loss: 0.32252\n",
            "EPOCH 67\n",
            " Train loss: 0.30781\n",
            "EPOCH 68\n",
            " Train loss: 0.29775\n",
            "EPOCH 69\n",
            " Train loss: 0.28979\n",
            "EPOCH 70\n",
            " Train loss: 0.28131\n",
            "EPOCH 71\n",
            " Train loss: 0.26777\n",
            "EPOCH 72\n",
            " Train loss: 0.25395\n",
            "EPOCH 73\n",
            " Train loss: 0.24391\n",
            "EPOCH 74\n",
            " Train loss: 0.23597\n",
            "EPOCH 75\n",
            " Train loss: 0.22836\n",
            "EPOCH 76\n",
            " Train loss: 0.21450\n",
            "EPOCH 77\n",
            " Train loss: 0.20125\n",
            "EPOCH 78\n",
            " Train loss: 0.18535\n",
            "EPOCH 79\n",
            " Train loss: 0.17278\n",
            "EPOCH 80\n",
            " Train loss: 0.16313\n",
            "EPOCH 81\n",
            " Train loss: 0.15516\n",
            "EPOCH 82\n",
            " Train loss: 0.15389\n",
            "EPOCH 83\n",
            " Train loss: 0.14207\n",
            "EPOCH 84\n",
            " Train loss: 0.13179\n",
            "EPOCH 85\n",
            " Train loss: 0.11779\n",
            "EPOCH 86\n",
            " Train loss: 0.11324\n",
            "EPOCH 87\n",
            " Train loss: 0.11695\n",
            "EPOCH 88\n",
            " Train loss: 0.10751\n",
            "EPOCH 89\n",
            " Train loss: 0.09851\n",
            "EPOCH 90\n",
            " Train loss: 0.08870\n",
            "EPOCH 91\n",
            " Train loss: 0.08812\n",
            "EPOCH 92\n",
            " Train loss: 0.08854\n",
            "EPOCH 93\n",
            " Train loss: 0.07546\n",
            "EPOCH 94\n",
            " Train loss: 0.07831\n",
            "EPOCH 95\n",
            " Train loss: 0.08211\n",
            "EPOCH 96\n",
            " Train loss: 0.06365\n",
            "EPOCH 97\n",
            " Train loss: 0.07806\n",
            "EPOCH 98\n",
            " Train loss: 0.07866\n",
            "EPOCH 99\n",
            " Train loss: 0.06621\n",
            "EPOCH 100\n",
            " Train loss: 0.06832\n",
            "EPOCH 101\n",
            " Train loss: 0.05073\n",
            "EPOCH 102\n",
            " Train loss: 0.05773\n",
            "EPOCH 103\n",
            " Train loss: 0.04811\n",
            "EPOCH 104\n",
            " Train loss: 0.04818\n",
            "EPOCH 105\n",
            " Train loss: 0.04025\n",
            "EPOCH 106\n",
            " Train loss: 0.04145\n",
            "EPOCH 107\n",
            " Train loss: 0.03356\n",
            "EPOCH 108\n",
            " Train loss: 0.03707\n",
            "EPOCH 109\n",
            " Train loss: 0.03412\n",
            "EPOCH 110\n",
            " Train loss: 0.02834\n",
            "EPOCH 111\n",
            " Train loss: 0.02958\n",
            "EPOCH 112\n",
            " Train loss: 0.02062\n",
            "EPOCH 113\n",
            " Train loss: 0.02345\n",
            "EPOCH 114\n",
            " Train loss: 0.01881\n",
            "EPOCH 115\n",
            " Train loss: 0.01931\n",
            "EPOCH 116\n",
            " Train loss: 0.01483\n",
            "EPOCH 117\n",
            " Train loss: 0.01632\n",
            "EPOCH 118\n",
            " Train loss: 0.01376\n",
            "EPOCH 119\n",
            " Train loss: 0.01346\n",
            "EPOCH 120\n",
            " Train loss: 0.01095\n",
            "EPOCH 121\n",
            " Train loss: 0.01125\n",
            "EPOCH 122\n",
            " Train loss: 0.01087\n",
            "EPOCH 123\n",
            " Train loss: 0.00937\n",
            "EPOCH 124\n",
            " Train loss: 0.00880\n",
            "EPOCH 125\n",
            " Train loss: 0.00804\n",
            "EPOCH 126\n",
            " Train loss: 0.00807\n",
            "EPOCH 127\n",
            " Train loss: 0.00781\n",
            "EPOCH 128\n",
            " Train loss: 0.00677\n",
            "EPOCH 129\n",
            " Train loss: 0.00630\n",
            "EPOCH 130\n",
            " Train loss: 0.00621\n",
            "EPOCH 131\n",
            " Train loss: 0.00588\n",
            "EPOCH 132\n",
            " Train loss: 0.00566\n",
            "EPOCH 133\n",
            " Train loss: 0.00539\n",
            "EPOCH 134\n",
            " Train loss: 0.00492\n",
            "EPOCH 135\n",
            " Train loss: 0.00467\n",
            "EPOCH 136\n",
            " Train loss: 0.00466\n",
            "EPOCH 137\n",
            " Train loss: 0.00449\n",
            "EPOCH 138\n",
            " Train loss: 0.00420\n",
            "EPOCH 139\n",
            " Train loss: 0.00403\n",
            "EPOCH 140\n",
            " Train loss: 0.00389\n",
            "EPOCH 141\n",
            " Train loss: 0.00371\n",
            "EPOCH 142\n",
            " Train loss: 0.00358\n",
            "EPOCH 143\n",
            " Train loss: 0.00352\n",
            "EPOCH 144\n",
            " Train loss: 0.00342\n",
            "EPOCH 145\n",
            " Train loss: 0.00325\n",
            "EPOCH 146\n",
            " Train loss: 0.00311\n",
            "EPOCH 147\n",
            " Train loss: 0.00304\n",
            "EPOCH 148\n",
            " Train loss: 0.00297\n",
            "EPOCH 149\n",
            " Train loss: 0.00288\n",
            "EPOCH 150\n",
            " Train loss: 0.00279\n",
            "EPOCH 151\n",
            " Train loss: 0.00272\n",
            "EPOCH 152\n",
            " Train loss: 0.00265\n",
            "EPOCH 153\n",
            " Train loss: 0.00256\n",
            "EPOCH 154\n",
            " Train loss: 0.00248\n",
            "EPOCH 155\n",
            " Train loss: 0.00243\n",
            "EPOCH 156\n",
            " Train loss: 0.00238\n",
            "EPOCH 157\n",
            " Train loss: 0.00233\n",
            "EPOCH 158\n",
            " Train loss: 0.00227\n",
            "EPOCH 159\n",
            " Train loss: 0.00221\n",
            "EPOCH 160\n",
            " Train loss: 0.00216\n",
            "EPOCH 161\n",
            " Train loss: 0.00212\n",
            "EPOCH 162\n",
            " Train loss: 0.00207\n",
            "EPOCH 163\n",
            " Train loss: 0.00203\n",
            "EPOCH 164\n",
            " Train loss: 0.00199\n",
            "EPOCH 165\n",
            " Train loss: 0.00195\n",
            "EPOCH 166\n",
            " Train loss: 0.00191\n",
            "EPOCH 167\n",
            " Train loss: 0.00187\n",
            "EPOCH 168\n",
            " Train loss: 0.00183\n",
            "EPOCH 169\n",
            " Train loss: 0.00180\n",
            "EPOCH 170\n",
            " Train loss: 0.00177\n",
            "EPOCH 171\n",
            " Train loss: 0.00174\n",
            "EPOCH 172\n",
            " Train loss: 0.00171\n",
            "EPOCH 173\n",
            " Train loss: 0.00168\n",
            "EPOCH 174\n",
            " Train loss: 0.00165\n",
            "EPOCH 175\n",
            " Train loss: 0.00162\n",
            "EPOCH 176\n",
            " Train loss: 0.00159\n",
            "EPOCH 177\n",
            " Train loss: 0.00157\n",
            "EPOCH 178\n",
            " Train loss: 0.00154\n",
            "EPOCH 179\n",
            " Train loss: 0.00152\n",
            "EPOCH 180\n",
            " Train loss: 0.00150\n",
            "EPOCH 181\n",
            " Train loss: 0.00147\n",
            "EPOCH 182\n",
            " Train loss: 0.00145\n",
            "EPOCH 183\n",
            " Train loss: 0.00143\n",
            "EPOCH 184\n",
            " Train loss: 0.00141\n",
            "EPOCH 185\n",
            " Train loss: 0.00139\n",
            "EPOCH 186\n",
            " Train loss: 0.00137\n",
            "EPOCH 187\n",
            " Train loss: 0.00135\n",
            "EPOCH 188\n",
            " Train loss: 0.00133\n",
            "EPOCH 189\n",
            " Train loss: 0.00131\n",
            "EPOCH 190\n",
            " Train loss: 0.00129\n",
            "EPOCH 191\n",
            " Train loss: 0.00128\n",
            "EPOCH 192\n",
            " Train loss: 0.00126\n",
            "EPOCH 193\n",
            " Train loss: 0.00124\n",
            "EPOCH 194\n",
            " Train loss: 0.00123\n",
            "EPOCH 195\n",
            " Train loss: 0.00121\n",
            "EPOCH 196\n",
            " Train loss: 0.00120\n",
            "EPOCH 197\n",
            " Train loss: 0.00118\n",
            "EPOCH 198\n",
            " Train loss: 0.00117\n",
            "EPOCH 199\n",
            " Train loss: 0.00115\n",
            "EPOCH 200\n",
            " Train loss: 0.00114\n",
            "EPOCH 201\n",
            " Train loss: 0.00112\n",
            "EPOCH 202\n",
            " Train loss: 0.00111\n",
            "EPOCH 203\n",
            " Train loss: 0.00110\n",
            "EPOCH 204\n",
            " Train loss: 0.00108\n",
            "EPOCH 205\n",
            " Train loss: 0.00107\n",
            "EPOCH 206\n",
            " Train loss: 0.00106\n",
            "EPOCH 207\n",
            " Train loss: 0.00105\n",
            "EPOCH 208\n",
            " Train loss: 0.00103\n",
            "EPOCH 209\n",
            " Train loss: 0.00102\n",
            "EPOCH 210\n",
            " Train loss: 0.00101\n",
            "EPOCH 211\n",
            " Train loss: 0.00100\n",
            "EPOCH 212\n",
            " Train loss: 0.00099\n",
            "EPOCH 213\n",
            " Train loss: 0.00098\n",
            "EPOCH 214\n",
            " Train loss: 0.00097\n",
            "EPOCH 215\n",
            " Train loss: 0.00096\n",
            "EPOCH 216\n",
            " Train loss: 0.00095\n",
            "EPOCH 217\n",
            " Train loss: 0.00094\n",
            "EPOCH 218\n",
            " Train loss: 0.00093\n",
            "EPOCH 219\n",
            " Train loss: 0.00092\n",
            "EPOCH 220\n",
            " Train loss: 0.00091\n",
            "EPOCH 221\n",
            " Train loss: 0.00090\n",
            "EPOCH 222\n",
            " Train loss: 0.00089\n",
            "EPOCH 223\n",
            " Train loss: 0.00088\n",
            "EPOCH 224\n",
            " Train loss: 0.00087\n",
            "EPOCH 225\n",
            " Train loss: 0.00086\n",
            "EPOCH 226\n",
            " Train loss: 0.00085\n",
            "EPOCH 227\n",
            " Train loss: 0.00084\n",
            "EPOCH 228\n",
            " Train loss: 0.00084\n",
            "EPOCH 229\n",
            " Train loss: 0.00083\n",
            "EPOCH 230\n",
            " Train loss: 0.00082\n",
            "EPOCH 231\n",
            " Train loss: 0.00081\n",
            "EPOCH 232\n",
            " Train loss: 0.00080\n",
            "EPOCH 233\n",
            " Train loss: 0.00080\n",
            "EPOCH 234\n",
            " Train loss: 0.00079\n",
            "EPOCH 235\n",
            " Train loss: 0.00078\n",
            "EPOCH 236\n",
            " Train loss: 0.00077\n",
            "EPOCH 237\n",
            " Train loss: 0.00077\n",
            "EPOCH 238\n",
            " Train loss: 0.00076\n",
            "EPOCH 239\n",
            " Train loss: 0.00075\n",
            "EPOCH 240\n",
            " Train loss: 0.00075\n",
            "EPOCH 241\n",
            " Train loss: 0.00074\n",
            "EPOCH 242\n",
            " Train loss: 0.00073\n",
            "EPOCH 243\n",
            " Train loss: 0.00073\n",
            "EPOCH 244\n",
            " Train loss: 0.00072\n",
            "EPOCH 245\n",
            " Train loss: 0.00071\n",
            "EPOCH 246\n",
            " Train loss: 0.00071\n",
            "EPOCH 247\n",
            " Train loss: 0.00070\n",
            "EPOCH 248\n",
            " Train loss: 0.00069\n",
            "EPOCH 249\n",
            " Train loss: 0.00069\n",
            "EPOCH 250\n",
            " Train loss: 0.00068\n",
            "EPOCH 251\n",
            " Train loss: 0.00068\n",
            "EPOCH 252\n",
            " Train loss: 0.00067\n",
            "EPOCH 253\n",
            " Train loss: 0.00066\n",
            "EPOCH 254\n",
            " Train loss: 0.00066\n",
            "EPOCH 255\n",
            " Train loss: 0.00065\n",
            "EPOCH 256\n",
            " Train loss: 0.00065\n",
            "EPOCH 257\n",
            " Train loss: 0.00064\n",
            "EPOCH 258\n",
            " Train loss: 0.00064\n",
            "EPOCH 259\n",
            " Train loss: 0.00063\n",
            "EPOCH 260\n",
            " Train loss: 0.00063\n",
            "EPOCH 261\n",
            " Train loss: 0.00062\n",
            "EPOCH 262\n",
            " Train loss: 0.00062\n",
            "EPOCH 263\n",
            " Train loss: 0.00061\n",
            "EPOCH 264\n",
            " Train loss: 0.00061\n",
            "EPOCH 265\n",
            " Train loss: 0.00060\n",
            "EPOCH 266\n",
            " Train loss: 0.00060\n",
            "EPOCH 267\n",
            " Train loss: 0.00059\n",
            "EPOCH 268\n",
            " Train loss: 0.00059\n",
            "EPOCH 269\n",
            " Train loss: 0.00058\n",
            "EPOCH 270\n",
            " Train loss: 0.00058\n",
            "EPOCH 271\n",
            " Train loss: 0.00057\n",
            "EPOCH 272\n",
            " Train loss: 0.00057\n",
            "EPOCH 273\n",
            " Train loss: 0.00057\n",
            "EPOCH 274\n",
            " Train loss: 0.00056\n",
            "EPOCH 275\n",
            " Train loss: 0.00056\n",
            "EPOCH 276\n",
            " Train loss: 0.00055\n",
            "EPOCH 277\n",
            " Train loss: 0.00055\n",
            "EPOCH 278\n",
            " Train loss: 0.00054\n",
            "EPOCH 279\n",
            " Train loss: 0.00054\n",
            "EPOCH 280\n",
            " Train loss: 0.00054\n",
            "EPOCH 281\n",
            " Train loss: 0.00053\n",
            "EPOCH 282\n",
            " Train loss: 0.00053\n",
            "EPOCH 283\n",
            " Train loss: 0.00052\n",
            "EPOCH 284\n",
            " Train loss: 0.00052\n",
            "EPOCH 285\n",
            " Train loss: 0.00052\n",
            "EPOCH 286\n",
            " Train loss: 0.00051\n",
            "EPOCH 287\n",
            " Train loss: 0.00051\n",
            "EPOCH 288\n",
            " Train loss: 0.00051\n",
            "EPOCH 289\n",
            " Train loss: 0.00050\n",
            "EPOCH 290\n",
            " Train loss: 0.00050\n",
            "EPOCH 291\n",
            " Train loss: 0.00050\n",
            "EPOCH 292\n",
            " Train loss: 0.00049\n",
            "EPOCH 293\n",
            " Train loss: 0.00049\n",
            "EPOCH 294\n",
            " Train loss: 0.00049\n",
            "EPOCH 295\n",
            " Train loss: 0.00048\n",
            "EPOCH 296\n",
            " Train loss: 0.00048\n",
            "EPOCH 297\n",
            " Train loss: 0.00048\n",
            "EPOCH 298\n",
            " Train loss: 0.00047\n",
            "EPOCH 299\n",
            " Train loss: 0.00047\n",
            "EPOCH 300\n",
            " Train loss: 0.00047\n",
            "EPOCH 301\n",
            " Train loss: 0.00046\n",
            "EPOCH 302\n",
            " Train loss: 0.00046\n",
            "EPOCH 303\n",
            " Train loss: 0.00046\n",
            "EPOCH 304\n",
            " Train loss: 0.00045\n",
            "EPOCH 305\n",
            " Train loss: 0.00045\n",
            "EPOCH 306\n",
            " Train loss: 0.00045\n",
            "EPOCH 307\n",
            " Train loss: 0.00044\n",
            "EPOCH 308\n",
            " Train loss: 0.00044\n",
            "EPOCH 309\n",
            " Train loss: 0.00044\n",
            "EPOCH 310\n",
            " Train loss: 0.00044\n",
            "EPOCH 311\n",
            " Train loss: 0.00043\n",
            "EPOCH 312\n",
            " Train loss: 0.00043\n",
            "EPOCH 313\n",
            " Train loss: 0.00043\n",
            "EPOCH 314\n",
            " Train loss: 0.00042\n",
            "EPOCH 315\n",
            " Train loss: 0.00042\n",
            "EPOCH 316\n",
            " Train loss: 0.00042\n",
            "EPOCH 317\n",
            " Train loss: 0.00042\n",
            "EPOCH 318\n",
            " Train loss: 0.00041\n",
            "EPOCH 319\n",
            " Train loss: 0.00041\n",
            "EPOCH 320\n",
            " Train loss: 0.00041\n",
            "EPOCH 321\n",
            " Train loss: 0.00041\n",
            "EPOCH 322\n",
            " Train loss: 0.00040\n",
            "EPOCH 323\n",
            " Train loss: 0.00040\n",
            "EPOCH 324\n",
            " Train loss: 0.00040\n",
            "EPOCH 325\n",
            " Train loss: 0.00040\n",
            "EPOCH 326\n",
            " Train loss: 0.00039\n",
            "EPOCH 327\n",
            " Train loss: 0.00039\n",
            "EPOCH 328\n",
            " Train loss: 0.00039\n",
            "EPOCH 329\n",
            " Train loss: 0.00039\n",
            "EPOCH 330\n",
            " Train loss: 0.00038\n",
            "EPOCH 331\n",
            " Train loss: 0.00038\n",
            "EPOCH 332\n",
            " Train loss: 0.00038\n",
            "EPOCH 333\n",
            " Train loss: 0.00038\n",
            "EPOCH 334\n",
            " Train loss: 0.00038\n",
            "EPOCH 335\n",
            " Train loss: 0.00037\n",
            "EPOCH 336\n",
            " Train loss: 0.00037\n",
            "EPOCH 337\n",
            " Train loss: 0.00037\n",
            "EPOCH 338\n",
            " Train loss: 0.00037\n",
            "EPOCH 339\n",
            " Train loss: 0.00036\n",
            "EPOCH 340\n",
            " Train loss: 0.00036\n",
            "EPOCH 341\n",
            " Train loss: 0.00036\n",
            "EPOCH 342\n",
            " Train loss: 0.00036\n",
            "EPOCH 343\n",
            " Train loss: 0.00036\n",
            "EPOCH 344\n",
            " Train loss: 0.00035\n",
            "EPOCH 345\n",
            " Train loss: 0.00035\n",
            "EPOCH 346\n",
            " Train loss: 0.00035\n",
            "EPOCH 347\n",
            " Train loss: 0.00035\n",
            "EPOCH 348\n",
            " Train loss: 0.00035\n",
            "EPOCH 349\n",
            " Train loss: 0.00034\n",
            "EPOCH 350\n",
            " Train loss: 0.00034\n",
            "EPOCH 351\n",
            " Train loss: 0.00034\n",
            "EPOCH 352\n",
            " Train loss: 0.00034\n",
            "EPOCH 353\n",
            " Train loss: 0.00034\n",
            "EPOCH 354\n",
            " Train loss: 0.00033\n",
            "EPOCH 355\n",
            " Train loss: 0.00033\n",
            "EPOCH 356\n",
            " Train loss: 0.00033\n",
            "EPOCH 357\n",
            " Train loss: 0.00033\n",
            "EPOCH 358\n",
            " Train loss: 0.00033\n",
            "EPOCH 359\n",
            " Train loss: 0.00033\n",
            "EPOCH 360\n",
            " Train loss: 0.00032\n",
            "EPOCH 361\n",
            " Train loss: 0.00032\n",
            "EPOCH 362\n",
            " Train loss: 0.00032\n",
            "EPOCH 363\n",
            " Train loss: 0.00032\n",
            "EPOCH 364\n",
            " Train loss: 0.00032\n",
            "EPOCH 365\n",
            " Train loss: 0.00031\n",
            "EPOCH 366\n",
            " Train loss: 0.00031\n",
            "EPOCH 367\n",
            " Train loss: 0.00031\n",
            "EPOCH 368\n",
            " Train loss: 0.00031\n",
            "EPOCH 369\n",
            " Train loss: 0.00031\n",
            "EPOCH 370\n",
            " Train loss: 0.00031\n",
            "EPOCH 371\n",
            " Train loss: 0.00031\n",
            "EPOCH 372\n",
            " Train loss: 0.00030\n",
            "EPOCH 373\n",
            " Train loss: 0.00030\n",
            "EPOCH 374\n",
            " Train loss: 0.00030\n",
            "EPOCH 375\n",
            " Train loss: 0.00030\n",
            "EPOCH 376\n",
            " Train loss: 0.00030\n",
            "EPOCH 377\n",
            " Train loss: 0.00030\n",
            "EPOCH 378\n",
            " Train loss: 0.00029\n",
            "EPOCH 379\n",
            " Train loss: 0.00029\n",
            "EPOCH 380\n",
            " Train loss: 0.00029\n",
            "EPOCH 381\n",
            " Train loss: 0.00029\n",
            "EPOCH 382\n",
            " Train loss: 0.00029\n",
            "EPOCH 383\n",
            " Train loss: 0.00029\n",
            "EPOCH 384\n",
            " Train loss: 0.00029\n",
            "EPOCH 385\n",
            " Train loss: 0.00028\n",
            "EPOCH 386\n",
            " Train loss: 0.00028\n",
            "EPOCH 387\n",
            " Train loss: 0.00028\n",
            "EPOCH 388\n",
            " Train loss: 0.00028\n",
            "EPOCH 389\n",
            " Train loss: 0.00028\n",
            "EPOCH 390\n",
            " Train loss: 0.00028\n",
            "EPOCH 391\n",
            " Train loss: 0.00028\n",
            "EPOCH 392\n",
            " Train loss: 0.00027\n",
            "EPOCH 393\n",
            " Train loss: 0.00027\n",
            "EPOCH 394\n",
            " Train loss: 0.00027\n",
            "EPOCH 395\n",
            " Train loss: 0.00027\n",
            "EPOCH 396\n",
            " Train loss: 0.00027\n",
            "EPOCH 397\n",
            " Train loss: 0.00027\n",
            "EPOCH 398\n",
            " Train loss: 0.00027\n",
            "EPOCH 399\n",
            " Train loss: 0.00027\n",
            "EPOCH 400\n",
            " Train loss: 0.00026\n",
            "EPOCH 401\n",
            " Train loss: 0.00026\n",
            "EPOCH 402\n",
            " Train loss: 0.00026\n",
            "EPOCH 403\n",
            " Train loss: 0.00026\n",
            "EPOCH 404\n",
            " Train loss: 0.00026\n",
            "EPOCH 405\n",
            " Train loss: 0.00026\n",
            "EPOCH 406\n",
            " Train loss: 0.00026\n",
            "EPOCH 407\n",
            " Train loss: 0.00026\n",
            "EPOCH 408\n",
            " Train loss: 0.00025\n",
            "EPOCH 409\n",
            " Train loss: 0.00025\n",
            "EPOCH 410\n",
            " Train loss: 0.00025\n",
            "EPOCH 411\n",
            " Train loss: 0.00025\n",
            "EPOCH 412\n",
            " Train loss: 0.00025\n",
            "EPOCH 413\n",
            " Train loss: 0.00025\n",
            "EPOCH 414\n",
            " Train loss: 0.00025\n",
            "EPOCH 415\n",
            " Train loss: 0.00025\n",
            "EPOCH 416\n",
            " Train loss: 0.00024\n",
            "EPOCH 417\n",
            " Train loss: 0.00024\n",
            "EPOCH 418\n",
            " Train loss: 0.00024\n",
            "EPOCH 419\n",
            " Train loss: 0.00024\n",
            "EPOCH 420\n",
            " Train loss: 0.00024\n",
            "EPOCH 421\n",
            " Train loss: 0.00024\n",
            "EPOCH 422\n",
            " Train loss: 0.00024\n",
            "EPOCH 423\n",
            " Train loss: 0.00024\n",
            "EPOCH 424\n",
            " Train loss: 0.00024\n",
            "EPOCH 425\n",
            " Train loss: 0.00023\n",
            "EPOCH 426\n",
            " Train loss: 0.00023\n",
            "EPOCH 427\n",
            " Train loss: 0.00023\n",
            "EPOCH 428\n",
            " Train loss: 0.00023\n",
            "EPOCH 429\n",
            " Train loss: 0.00023\n",
            "EPOCH 430\n",
            " Train loss: 0.00023\n",
            "EPOCH 431\n",
            " Train loss: 0.00023\n",
            "EPOCH 432\n",
            " Train loss: 0.00023\n",
            "EPOCH 433\n",
            " Train loss: 0.00023\n",
            "EPOCH 434\n",
            " Train loss: 0.00023\n",
            "EPOCH 435\n",
            " Train loss: 0.00022\n",
            "EPOCH 436\n",
            " Train loss: 0.00022\n",
            "EPOCH 437\n",
            " Train loss: 0.00022\n",
            "EPOCH 438\n",
            " Train loss: 0.00022\n",
            "EPOCH 439\n",
            " Train loss: 0.00022\n",
            "EPOCH 440\n",
            " Train loss: 0.00022\n",
            "EPOCH 441\n",
            " Train loss: 0.00022\n",
            "EPOCH 442\n",
            " Train loss: 0.00022\n",
            "EPOCH 443\n",
            " Train loss: 0.00022\n",
            "EPOCH 444\n",
            " Train loss: 0.00022\n",
            "EPOCH 445\n",
            " Train loss: 0.00022\n",
            "EPOCH 446\n",
            " Train loss: 0.00021\n",
            "EPOCH 447\n",
            " Train loss: 0.00021\n",
            "EPOCH 448\n",
            " Train loss: 0.00021\n",
            "EPOCH 449\n",
            " Train loss: 0.00021\n",
            "EPOCH 450\n",
            " Train loss: 0.00021\n",
            "EPOCH 451\n",
            " Train loss: 0.00021\n",
            "EPOCH 452\n",
            " Train loss: 0.00021\n",
            "EPOCH 453\n",
            " Train loss: 0.00021\n",
            "EPOCH 454\n",
            " Train loss: 0.00021\n",
            "EPOCH 455\n",
            " Train loss: 0.00021\n",
            "EPOCH 456\n",
            " Train loss: 0.00021\n",
            "EPOCH 457\n",
            " Train loss: 0.00020\n",
            "EPOCH 458\n",
            " Train loss: 0.00020\n",
            "EPOCH 459\n",
            " Train loss: 0.00020\n",
            "EPOCH 460\n",
            " Train loss: 0.00020\n",
            "EPOCH 461\n",
            " Train loss: 0.00020\n",
            "EPOCH 462\n",
            " Train loss: 0.00020\n",
            "EPOCH 463\n",
            " Train loss: 0.00020\n",
            "EPOCH 464\n",
            " Train loss: 0.00020\n",
            "EPOCH 465\n",
            " Train loss: 0.00020\n",
            "EPOCH 466\n",
            " Train loss: 0.00020\n",
            "EPOCH 467\n",
            " Train loss: 0.00020\n",
            "EPOCH 468\n",
            " Train loss: 0.00020\n",
            "EPOCH 469\n",
            " Train loss: 0.00019\n",
            "EPOCH 470\n",
            " Train loss: 0.00019\n",
            "EPOCH 471\n",
            " Train loss: 0.00019\n",
            "EPOCH 472\n",
            " Train loss: 0.00019\n",
            "EPOCH 473\n",
            " Train loss: 0.00019\n",
            "EPOCH 474\n",
            " Train loss: 0.00019\n",
            "EPOCH 475\n",
            " Train loss: 0.00019\n",
            "EPOCH 476\n",
            " Train loss: 0.00019\n",
            "EPOCH 477\n",
            " Train loss: 0.00019\n",
            "EPOCH 478\n",
            " Train loss: 0.00019\n",
            "EPOCH 479\n",
            " Train loss: 0.00019\n",
            "EPOCH 480\n",
            " Train loss: 0.00019\n",
            "EPOCH 481\n",
            " Train loss: 0.00019\n",
            "EPOCH 482\n",
            " Train loss: 0.00018\n",
            "EPOCH 483\n",
            " Train loss: 0.00018\n",
            "EPOCH 484\n",
            " Train loss: 0.00018\n",
            "EPOCH 485\n",
            " Train loss: 0.00018\n",
            "EPOCH 486\n",
            " Train loss: 0.00018\n",
            "EPOCH 487\n",
            " Train loss: 0.00018\n",
            "EPOCH 488\n",
            " Train loss: 0.00018\n",
            "EPOCH 489\n",
            " Train loss: 0.00018\n",
            "EPOCH 490\n",
            " Train loss: 0.00018\n",
            "EPOCH 491\n",
            " Train loss: 0.00018\n",
            "EPOCH 492\n",
            " Train loss: 0.00018\n",
            "EPOCH 493\n",
            " Train loss: 0.00018\n",
            "EPOCH 494\n",
            " Train loss: 0.00018\n",
            "EPOCH 495\n",
            " Train loss: 0.00018\n",
            "EPOCH 496\n",
            " Train loss: 0.00018\n",
            "EPOCH 497\n",
            " Train loss: 0.00017\n",
            "EPOCH 498\n",
            " Train loss: 0.00017\n",
            "EPOCH 499\n",
            " Train loss: 0.00017\n"
          ]
        }
      ],
      "source": [
        "train_network(model, train_loader, criterion, optimizer, nepoch=500)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 94.444\n"
          ]
        }
      ],
      "source": [
        "acc, true, pred = test_network(model, test_loader)\n",
        "# print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'my_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "age                                       45\n",
            "time_in_hospital                           1\n",
            "num_lab_procedures                        45\n",
            "num_procedures                             0\n",
            "num_medications                           12\n",
            "number_diagnoses                           3\n",
            "change                                     0\n",
            "diabetesMed                                0\n",
            "readmitted                                 1\n",
            "admission_type_id_Elective                 0\n",
            "admission_type_id_Emergency                0\n",
            "admission_type_id_Other                    1\n",
            "discharge_disposition_id_Home              0\n",
            "discharge_disposition_id_Other             1\n",
            "admission_source_id_Emergency Room         0\n",
            "admission_source_id_Physician Referral     1\n",
            "metformin_Down                             0\n",
            "metformin_No                               1\n",
            "metformin_Steady                           0\n",
            "metformin_Up                               0\n",
            "insulin_Down                               0\n",
            "insulin_No                                 1\n",
            "insulin_Steady                             0\n",
            "insulin_Up                                 0\n",
            "diag_1_Circulatory                         0\n",
            "diag_1_Digestive                           0\n",
            "diag_1_Endocrine                           0\n",
            "diag_1_Genitourinary                       0\n",
            "diag_1_Infectious                          0\n",
            "diag_1_Musculoskeletal                     0\n",
            "diag_1_Neoplasms                           0\n",
            "diag_1_Nervous                             0\n",
            "diag_1_Other                               1\n",
            "diag_1_Respiratory                         0\n",
            "diag_1_Symptoms                            0\n",
            "gender_Female                              1\n",
            "gender_Male                                0\n",
            "max_glu_serum_>200                         0\n",
            "max_glu_serum_>300                         0\n",
            "max_glu_serum_Norm                         1\n",
            "A1Cresult_>7                               0\n",
            "A1Cresult_>8                               0\n",
            "A1Cresult_Norm                             1\n",
            "Name: 120, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#print(X.df.columns)\n",
        "print(X.df[X.df['readmitted'] == 1].iloc[5])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
